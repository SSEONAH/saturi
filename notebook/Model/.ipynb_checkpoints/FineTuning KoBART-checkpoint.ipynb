{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53976b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "622d22e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = os.getenv('HOME') + '/aiffel/aiffelthon/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bf0be9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b538a9d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['reg_1.json',\n",
       " 'meta_data_included_raw_v2.csv',\n",
       " '.ipynb_checkpoints',\n",
       " 'meta_eng_raw_0306_v1.csv',\n",
       " 'kor_text.xlsx',\n",
       " 'reg_3.json',\n",
       " 'meta_data_included_raw.csv',\n",
       " 'meta_raw_eng_corrected_sts_v1.csv',\n",
       " 'meta_data_raw_eng_match_6m.csv',\n",
       " 'kor_text.txt',\n",
       " 'meta_raw_eng_sts_v1.csv',\n",
       " 'reg_4.json',\n",
       " 'meta_raw_eng_corrected_sts_v1_needs_correction_translated.xlsx',\n",
       " 'meta_raw_eng_corrected_sts_v1_needs_correction.xlsx',\n",
       " 'kw_t.json',\n",
       " 'reg_5.json',\n",
       " 'meta_raw_eng_corrected_sts_v_final.csv',\n",
       " 'meta_data_raw_0306.csv',\n",
       " 'reg_2.json']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(directory + \"meta/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03d6b089",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(directory + \"meta/meta_raw_eng_corrected_sts_v_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89794de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "del df['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df5d2b4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reg</th>\n",
       "      <th>topic</th>\n",
       "      <th>stdn</th>\n",
       "      <th>dial</th>\n",
       "      <th>eng</th>\n",
       "      <th>eojeol_stdn</th>\n",
       "      <th>sts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jd</td>\n",
       "      <td>역사</td>\n",
       "      <td>생각이 쪼금씩 바뀌더라고</td>\n",
       "      <td>생각이 쪼금씩 바뀌드라고</td>\n",
       "      <td>I've changed my mind a little bit.</td>\n",
       "      <td>[['(바뀌드라고)/(바뀌더라고)']]</td>\n",
       "      <td>0.056177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jd</td>\n",
       "      <td>가족</td>\n",
       "      <td>어 알겠는가 외국인들이 그래도 잘 적응하고</td>\n",
       "      <td>어 알겄는가 외국인들이 그래도 잘 적응하고</td>\n",
       "      <td>You know what? Foreigners still get used to it.</td>\n",
       "      <td>[['(알겄는가)/(알겠는가)']]</td>\n",
       "      <td>-0.006247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jd</td>\n",
       "      <td>건강</td>\n",
       "      <td>아으 갑자기 예전에 맹장 뜯은 게 생각난다.</td>\n",
       "      <td>아으 갑자기 예전에 맹장 뜯은 게 생각난디야.</td>\n",
       "      <td>All of a sudden, I think I've ripped off my ap...</td>\n",
       "      <td>[['(생각난디야.)/(생각난다.)']]</td>\n",
       "      <td>0.174105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jd</td>\n",
       "      <td>스타일</td>\n",
       "      <td>보면은 조금 품위 있게 나이 들어가야 되겠단 생각을 참 많이 해요.</td>\n",
       "      <td>보면은 쫌 품위 있게 나이 들어가야 되겠단 생각을 참 많이 해요.</td>\n",
       "      <td>I have a lot of ideas about getting older in a...</td>\n",
       "      <td>[['(쫌)/(조금)']]</td>\n",
       "      <td>0.120993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jd</td>\n",
       "      <td>먹거리</td>\n",
       "      <td>약간 조금 약간 맛이 쪼끔 거시기 하긴 한데 그래도</td>\n",
       "      <td>약간 쫌 약간 맛이 쪼끔 거시기 하긴 한데 그래도</td>\n",
       "      <td>It's a little bit of a tastey, but still...</td>\n",
       "      <td>[['(쫌)/(조금)']]</td>\n",
       "      <td>0.256973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>949883</th>\n",
       "      <td>jd</td>\n",
       "      <td>다이어트</td>\n",
       "      <td>뭐~ 맨날 집에서 인제 음식도 그냥 가까운 데 나가서 먹는 게 아니라 되게 시켜먹는...</td>\n",
       "      <td>뭐~ 맨날 집에서 인제 음식도 그냥 가까운 데 나가서 먹는 게 아니라 되게 시켜먹는...</td>\n",
       "      <td>I suppose I dont just go out to the nearest pl...</td>\n",
       "      <td>[['(쫌)/(조금)']]</td>\n",
       "      <td>0.047894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>949884</th>\n",
       "      <td>gs</td>\n",
       "      <td>4차산업혁명</td>\n",
       "      <td>너는 만약에 그런 시대가 오면 네가 어떻게 활동할 것 같애?</td>\n",
       "      <td>너는 만약에 그런 시대가 오면 니가 어떻게 활동할 것 같애?</td>\n",
       "      <td>How do you think you will be active when that ...</td>\n",
       "      <td>[['(니가)/(네가)']]</td>\n",
       "      <td>0.142204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>949885</th>\n",
       "      <td>jd</td>\n",
       "      <td>만화</td>\n",
       "      <td>하이브도 있고 #조석 작가 이제 조의 영역이라는 웹툰도 있고 마음의 소리도 있는데</td>\n",
       "      <td>하이브도 있고 조석 작가 이제 조의 영역이라는 웹툰도 있고 마음의 소리도 있는디</td>\n",
       "      <td>There is a hive a webtoon called the area of J...</td>\n",
       "      <td>[['(조석)/(#조석)', '(있는디)/(있는데)']]</td>\n",
       "      <td>0.282759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>949886</th>\n",
       "      <td>jj</td>\n",
       "      <td>반려동물</td>\n",
       "      <td>추석도 친척들도 사촌까지만 모이긴 했는데 이~ 다 모이니까 할 말도 많고 이~</td>\n",
       "      <td>추석도 친척들도 사촌까지만 모이긴 했는데 이~ 다 모이난이 할 말도 많고 이~</td>\n",
       "      <td>On Chuseok relatives and cousins only gathered...</td>\n",
       "      <td>[['(모이난이)/(모이니까)']]</td>\n",
       "      <td>-0.043421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>949887</th>\n",
       "      <td>cc</td>\n",
       "      <td>먹거리</td>\n",
       "      <td>커피숍 가서 이렇게 먹고 하는 거가 훨씬 맛있거든 기분상으로도 그렇고</td>\n",
       "      <td>커피숍 가서 이케 먹고 하는 거가 훨씬 맛있거든 기분상으로도 그렇고</td>\n",
       "      <td>Its much better to eat at a coffee shop and it...</td>\n",
       "      <td>[['(이케)/(이렇게)']]</td>\n",
       "      <td>0.145249</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>949888 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       reg   topic                                               stdn  \\\n",
       "0       jd      역사                                      생각이 쪼금씩 바뀌더라고   \n",
       "1       jd      가족                            어 알겠는가 외국인들이 그래도 잘 적응하고   \n",
       "2       jd      건강                           아으 갑자기 예전에 맹장 뜯은 게 생각난다.   \n",
       "3       jd     스타일              보면은 조금 품위 있게 나이 들어가야 되겠단 생각을 참 많이 해요.   \n",
       "4       jd     먹거리                       약간 조금 약간 맛이 쪼끔 거시기 하긴 한데 그래도   \n",
       "...     ..     ...                                                ...   \n",
       "949883  jd    다이어트  뭐~ 맨날 집에서 인제 음식도 그냥 가까운 데 나가서 먹는 게 아니라 되게 시켜먹는...   \n",
       "949884  gs  4차산업혁명                  너는 만약에 그런 시대가 오면 네가 어떻게 활동할 것 같애?   \n",
       "949885  jd      만화      하이브도 있고 #조석 작가 이제 조의 영역이라는 웹툰도 있고 마음의 소리도 있는데   \n",
       "949886  jj    반려동물        추석도 친척들도 사촌까지만 모이긴 했는데 이~ 다 모이니까 할 말도 많고 이~   \n",
       "949887  cc     먹거리             커피숍 가서 이렇게 먹고 하는 거가 훨씬 맛있거든 기분상으로도 그렇고   \n",
       "\n",
       "                                                     dial  \\\n",
       "0                                           생각이 쪼금씩 바뀌드라고   \n",
       "1                                 어 알겄는가 외국인들이 그래도 잘 적응하고   \n",
       "2                               아으 갑자기 예전에 맹장 뜯은 게 생각난디야.   \n",
       "3                    보면은 쫌 품위 있게 나이 들어가야 되겠단 생각을 참 많이 해요.   \n",
       "4                             약간 쫌 약간 맛이 쪼끔 거시기 하긴 한데 그래도   \n",
       "...                                                   ...   \n",
       "949883  뭐~ 맨날 집에서 인제 음식도 그냥 가까운 데 나가서 먹는 게 아니라 되게 시켜먹는...   \n",
       "949884                  너는 만약에 그런 시대가 오면 니가 어떻게 활동할 것 같애?   \n",
       "949885       하이브도 있고 조석 작가 이제 조의 영역이라는 웹툰도 있고 마음의 소리도 있는디   \n",
       "949886        추석도 친척들도 사촌까지만 모이긴 했는데 이~ 다 모이난이 할 말도 많고 이~   \n",
       "949887              커피숍 가서 이케 먹고 하는 거가 훨씬 맛있거든 기분상으로도 그렇고   \n",
       "\n",
       "                                                      eng  \\\n",
       "0                      I've changed my mind a little bit.   \n",
       "1         You know what? Foreigners still get used to it.   \n",
       "2       All of a sudden, I think I've ripped off my ap...   \n",
       "3       I have a lot of ideas about getting older in a...   \n",
       "4             It's a little bit of a tastey, but still...   \n",
       "...                                                   ...   \n",
       "949883  I suppose I dont just go out to the nearest pl...   \n",
       "949884  How do you think you will be active when that ...   \n",
       "949885  There is a hive a webtoon called the area of J...   \n",
       "949886  On Chuseok relatives and cousins only gathered...   \n",
       "949887  Its much better to eat at a coffee shop and it...   \n",
       "\n",
       "                            eojeol_stdn       sts  \n",
       "0                 [['(바뀌드라고)/(바뀌더라고)']]  0.056177  \n",
       "1                   [['(알겄는가)/(알겠는가)']] -0.006247  \n",
       "2                [['(생각난디야.)/(생각난다.)']]  0.174105  \n",
       "3                        [['(쫌)/(조금)']]  0.120993  \n",
       "4                        [['(쫌)/(조금)']]  0.256973  \n",
       "...                                 ...       ...  \n",
       "949883                   [['(쫌)/(조금)']]  0.047894  \n",
       "949884                  [['(니가)/(네가)']]  0.142204  \n",
       "949885  [['(조석)/(#조석)', '(있는디)/(있는데)']]  0.282759  \n",
       "949886              [['(모이난이)/(모이니까)']] -0.043421  \n",
       "949887                 [['(이케)/(이렇게)']]  0.145249  \n",
       "\n",
       "[949888 rows x 7 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "520fa4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['eng_v'] = '<'+df['reg']+'> ' + df['eng'] \n",
    "df['dial_v'] = '<'+df['reg']+'> ' + df['dial'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35a33d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = datasets.Dataset.from_pandas(df.loc[:1000,['eng_v','dial_v']].sample(frac=1))\n",
    "test =  datasets.Dataset.from_pandas(df.loc[:100,['eng_v','dial_v']].sample(frac=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc1ecabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.DatasetDict({\"train\":train, \"test\": test})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac7f69e",
   "metadata": {},
   "source": [
    "# Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd50d75",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a64fd59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, AutoConfig,BartForConditionalGeneration,BartPretrainedModel, BartModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96e682f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_ckpt = 'circulus/kobart-trans-en-ko-v2'\n",
    "config = AutoConfig.from_pretrained(model_ckpt)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_ckpt, config=config)\n",
    "tokenizer= AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08915346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(30005, 768)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tokens = ['<jj>','<jd>','<cc>','<gs>','<kw>']\n",
    "tokenizer.add_tokens(new_tokens)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5517570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartConfig {\n",
       "  \"_name_or_path\": \"circulus/kobart-trans-en-ko-v2\",\n",
       "  \"activation_dropout\": 0.0,\n",
       "  \"activation_function\": \"gelu\",\n",
       "  \"add_bias_logits\": false,\n",
       "  \"add_final_layer_norm\": false,\n",
       "  \"architectures\": [\n",
       "    \"BartForConditionalGeneration\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
       "  \"bos_token_id\": 1,\n",
       "  \"classif_dropout\": 0.1,\n",
       "  \"classifier_dropout\": 0.1,\n",
       "  \"d_model\": 768,\n",
       "  \"decoder_attention_heads\": 16,\n",
       "  \"decoder_ffn_dim\": 3072,\n",
       "  \"decoder_layerdrop\": 0.0,\n",
       "  \"decoder_layers\": 6,\n",
       "  \"decoder_start_token_id\": 1,\n",
       "  \"do_blenderbot_90_layernorm\": false,\n",
       "  \"dropout\": 0.1,\n",
       "  \"encoder_attention_heads\": 16,\n",
       "  \"encoder_ffn_dim\": 3072,\n",
       "  \"encoder_layerdrop\": 0.0,\n",
       "  \"encoder_layers\": 6,\n",
       "  \"eos_token_id\": 1,\n",
       "  \"extra_pos_embeddings\": 2,\n",
       "  \"force_bos_token_to_be_generated\": false,\n",
       "  \"forced_eos_token_id\": 1,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"NEGATIVE\",\n",
       "    \"1\": \"POSITIVE\"\n",
       "  },\n",
       "  \"init_std\": 0.02,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"kobart_version\": 2.0,\n",
       "  \"label2id\": {\n",
       "    \"NEGATIVE\": 0,\n",
       "    \"POSITIVE\": 1\n",
       "  },\n",
       "  \"max_position_embeddings\": 1026,\n",
       "  \"model_type\": \"bart\",\n",
       "  \"normalize_before\": false,\n",
       "  \"normalize_embedding\": true,\n",
       "  \"num_hidden_layers\": 6,\n",
       "  \"pad_token_id\": 3,\n",
       "  \"scale_embedding\": false,\n",
       "  \"static_position_embeddings\": false,\n",
       "  \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.11.3\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30005\n",
       "}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e9234131",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_outputs import Seq2SeqLMOutput\n",
    "\n",
    "class BartForConditionalGeneration(BartPretrainedModel) :\n",
    "    config_class = config\n",
    "    \n",
    "    def __init__(self, config) :\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.vocab_size\n",
    "        self.bart = BartModel(config)\n",
    "        self.lm_head = torch.nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(self, input_ids=None, \n",
    "                attention_mask =None, \n",
    "                decoder_input_ids=None, \n",
    "                decoder_attention_mask=None,\n",
    "                labels=None,**kwargs) :\n",
    "        \n",
    "        outputs = self.bart(input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            decoder_input_ids = decoder_input_ids, \n",
    "                            decoder_attention_mask = decoder_attention_mask,\n",
    "                            **kwargs)\n",
    "        \n",
    "        lm_logits = self.lm_head(outputs[0])\n",
    "        \n",
    "        lm_loss = None\n",
    "        \n",
    "        if labels is not None :\n",
    "            loss_fct = torch.nn.CrossEntropyLoss()\n",
    "            lm_loss = loss_fct(lm_logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            \n",
    "        return Seq2SeqLMOutput(\n",
    "            loss=lm_loss,\n",
    "            logits=lm_logits,\n",
    "            past_key_values=outputs.past_key_values,\n",
    "            decoder_hidden_states=outputs.decoder_hidden_states,\n",
    "            decoder_attentions=outputs.decoder_attentions,\n",
    "            cross_attentions=outputs.cross_attentions,\n",
    "            encoder_last_hidden_state=outputs.encoder_last_hidden_state,\n",
    "            encoder_hidden_states=outputs.encoder_hidden_states,\n",
    "            encoder_attentions=outputs.encoder_attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab191d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.bart.modeling_bart import shift_tokens_right\n",
    "\n",
    "max_length = 768\n",
    "def convert_to_features(example) :\n",
    "    enc_input = tokenizer.batch_encode_plus(example['eng_v'], max_length=max_length, padding='max_length')\n",
    "    target_input = tokenizer.batch_encode_plus(example['dial_v'], max_length=max_length, padding='max_length')\n",
    "    dec_input = shift_tokens_right(torch.tensor(target_input['input_ids']),tokenizer.pad_token_id,tokenizer.bos_token_id).tolist()\n",
    "    dec_attention_input = shift_tokens_right(torch.tensor(target_input['input_ids']),tokenizer.pad_token_id,tokenizer.bos_token_id).tolist()\n",
    "    \n",
    "    encoding = {\n",
    "        'input_ids' : enc_input['input_ids'],\n",
    "        'attention_mask' : enc_input['attention_mask'],\n",
    "        'decoder_input_ids' : dec_input,\n",
    "        'decoder_attention_mask' : dec_attention_input,\n",
    "        'labels' : target_input['input_ids']\n",
    "    }\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "783f386a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66594c70cbec4826890dab0550bb1e20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c129ff6e448042daba7b4d81408da873",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_test = dataset.map(convert_to_features,batched=True, remove_columns =dataset['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4562b83c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['attention_mask', 'decoder_attention_mask', 'decoder_input_ids', 'input_ids', 'labels'],\n",
       "        num_rows: 1001\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['attention_mask', 'decoder_attention_mask', 'decoder_input_ids', 'input_ids', 'labels'],\n",
       "        num_rows: 101\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "33380d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_test.set_format('torch')\n",
    "# input_test = dataset_test['train'][:4]\n",
    "# lm_labels = input_test['labels']\n",
    "# del input_test['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fcfe355e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bart = BartModel(config)\n",
    "# lm_head = torch.nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
    "# output = bart(**input_test)\n",
    "# output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e24678bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lm_logits = lm_head(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d616c63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lm_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e08cd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_fct = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2692740e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lm_loss = loss_fct(lm_logits.view(-1, config.vocab_size), lm_labels.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "33ff5c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lm_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61dad63",
   "metadata": {},
   "source": [
    "### Preprocessing for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "39a3804b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219b4198",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f32ed246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ! pip install --upgrade huggingface_hub \n",
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d5e88104",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1e81d078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting sacrebleu\n",
      "  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n",
      "     |████████████████████████████████| 118 kB 6.7 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from sacrebleu) (1.21.4)\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.9/site-packages (from sacrebleu) (2021.11.10)\n",
      "Requirement already satisfied: colorama in /opt/conda/lib/python3.9/site-packages (from sacrebleu) (0.4.4)\n",
      "Requirement already satisfied: portalocker in /opt/conda/lib/python3.9/site-packages (from sacrebleu) (2.3.2)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.9/site-packages (from sacrebleu) (0.8.9)\n",
      "Requirement already satisfied: lxml in /opt/conda/lib/python3.9/site-packages (from sacrebleu) (4.6.3)\n",
      "Installing collected packages: sacrebleu\n",
      "Successfully installed sacrebleu-2.3.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "15f178e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7090dc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "98c6b728",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init():\n",
    "    return (BartForConditionalGeneration\n",
    "            .from_pretrained(model_ckpt, config= config)\n",
    "            .to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9062f6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "model_name = 'BART'\n",
    "source_lang= 'eng'\n",
    "target_lang = 'dial'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3bdad550",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "366776e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Seq2SeqTrainingArguments(\n",
    "    f\"{model_name}-finetuned-{source_lang}-to-{target_lang}\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=1,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e891b3bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file https://huggingface.co/circulus/kobart-trans-en-ko-v2/resolve/main/pytorch_model.bin from cache at /aiffel/.cache/huggingface/transformers/37376455ce67919ebfbfd84032875e0989f86be4686adbccec2993d3fafe29c8.0e5b5adc2067434ee15e3a7669c833bd6f2979558cfcb531c8f9e99ad40dae33\n",
      "Some weights of the model checkpoint at circulus/kobart-trans-en-ko-v2 were not used when initializing BartForConditionalGeneration: ['model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.decoder.layers.4.self_attn.v_proj.bias', 'model.decoder.layers.3.encoder_attn.k_proj.weight', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.decoder.layers.0.self_attn.out_proj.weight', 'model.decoder.layers.4.self_attn.q_proj.bias', 'model.decoder.layers.0.self_attn.q_proj.bias', 'model.decoder.layers.4.self_attn_layer_norm.bias', 'model.decoder.layers.3.fc2.bias', 'model.decoder.layers.1.fc2.weight', 'model.decoder.layers.2.self_attn.v_proj.weight', 'model.decoder.layers.2.self_attn_layer_norm.bias', 'model.decoder.layers.2.encoder_attn.k_proj.bias', 'model.decoder.layers.4.encoder_attn.q_proj.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.decoder.layers.0.encoder_attn.v_proj.bias', 'model.decoder.layers.0.final_layer_norm.weight', 'model.decoder.layers.1.encoder_attn.k_proj.weight', 'model.decoder.layers.3.fc1.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.decoder.layers.4.encoder_attn.out_proj.weight', 'model.decoder.layers.2.encoder_attn_layer_norm.bias', 'model.decoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.decoder.layers.2.self_attn.q_proj.bias', 'model.decoder.layers.3.self_attn.v_proj.bias', 'model.decoder.layers.0.fc2.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.decoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.decoder.layers.1.self_attn_layer_norm.bias', 'model.decoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.decoder.layers.3.encoder_attn_layer_norm.weight', 'model.decoder.layers.1.encoder_attn.q_proj.weight', 'model.decoder.layers.3.encoder_attn.k_proj.bias', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.decoder.layers.1.encoder_attn.k_proj.bias', 'model.decoder.layers.3.encoder_attn.q_proj.bias', 'model.decoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.decoder.layers.4.self_attn.k_proj.bias', 'model.decoder.layers.0.self_attn_layer_norm.bias', 'model.decoder.layers.5.final_layer_norm.weight', 'model.decoder.layers.2.fc1.weight', 'model.encoder.layers.1.fc1.weight', 'model.decoder.layers.0.encoder_attn.q_proj.weight', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.embed_positions.weight', 'model.decoder.layers.2.fc1.bias', 'model.encoder.layers.3.final_layer_norm.bias', 'model.decoder.layernorm_embedding.weight', 'model.decoder.layers.2.encoder_attn.v_proj.bias', 'model.decoder.layers.4.fc1.bias', 'model.decoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.decoder.layers.0.encoder_attn.out_proj.bias', 'model.decoder.layernorm_embedding.bias', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.decoder.layers.0.encoder_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.weight', 'model.decoder.layers.1.self_attn.k_proj.weight', 'model.decoder.layers.4.encoder_attn_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.bias', 'model.decoder.layers.2.encoder_attn.k_proj.weight', 'model.decoder.layers.4.self_attn_layer_norm.weight', 'model.decoder.layers.5.encoder_attn.v_proj.bias', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.decoder.layers.1.fc2.bias', 'model.decoder.layers.3.encoder_attn.out_proj.bias', 'model.decoder.layers.2.fc2.weight', 'model.encoder.layers.5.fc1.weight', 'model.decoder.layers.0.fc1.bias', 'model.decoder.layers.3.self_attn.k_proj.weight', 'model.decoder.layers.0.encoder_attn.q_proj.bias', 'model.encoder.layers.5.final_layer_norm.weight', 'model.decoder.layers.4.encoder_attn_layer_norm.bias', 'model.decoder.layers.5.encoder_attn.v_proj.weight', 'model.decoder.layers.0.final_layer_norm.bias', 'model.decoder.layers.1.encoder_attn.out_proj.weight', 'model.encoder.layers.2.fc2.bias', 'model.decoder.layers.3.encoder_attn_layer_norm.bias', 'model.decoder.layers.0.encoder_attn_layer_norm.weight', 'model.decoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.decoder.layers.4.encoder_attn.k_proj.bias', 'model.decoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.decoder.layers.3.self_attn.k_proj.bias', 'model.decoder.layers.5.fc1.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.4.self_attn.v_proj.bias', 'model.decoder.layers.4.encoder_attn.q_proj.bias', 'model.decoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.decoder.layers.2.encoder_attn.q_proj.bias', 'model.decoder.layers.3.encoder_attn.q_proj.weight', 'model.decoder.layers.3.self_attn_layer_norm.weight', 'model.decoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.decoder.layers.1.encoder_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.decoder.layers.0.self_attn.out_proj.bias', 'model.decoder.layers.2.self_attn.out_proj.bias', 'model.decoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.2.fc1.bias', 'model.decoder.layers.0.fc2.bias', 'model.decoder.layers.5.encoder_attn.q_proj.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.decoder.layers.4.fc2.bias', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.decoder.layers.1.self_attn.out_proj.bias', 'model.decoder.layers.5.fc2.weight', 'model.decoder.layers.0.fc1.weight', 'model.decoder.layers.5.encoder_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.decoder.layers.0.encoder_attn.k_proj.weight', 'model.decoder.layers.1.encoder_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.decoder.layers.5.encoder_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.0.fc1.bias', 'model.decoder.layers.3.encoder_attn.v_proj.bias', 'model.decoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.decoder.layers.5.self_attn.out_proj.weight', 'model.decoder.layers.5.fc1.bias', 'model.decoder.embed_positions.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.embed_tokens.weight', 'model.decoder.layers.1.encoder_attn.out_proj.bias', 'model.decoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.decoder.layers.5.encoder_attn.out_proj.bias', 'model.decoder.layers.0.encoder_attn.out_proj.weight', 'model.decoder.layers.4.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.decoder.layers.1.encoder_attn_layer_norm.bias', 'model.decoder.embed_tokens.weight', 'model.decoder.layers.3.self_attn.out_proj.bias', 'model.decoder.layers.2.encoder_attn.v_proj.weight', 'model.decoder.layers.1.self_attn.v_proj.bias', 'model.decoder.layers.3.self_attn.q_proj.weight', 'model.decoder.layers.4.fc1.weight', 'model.decoder.layers.1.self_attn.v_proj.weight', 'model.decoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.decoder.layers.5.fc2.bias', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.decoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layernorm_embedding.weight', 'model.decoder.layers.2.encoder_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.decoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.4.fc1.bias', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.0.fc2.bias', 'model.decoder.layers.0.encoder_attn_layer_norm.bias', 'model.decoder.layers.1.self_attn.q_proj.weight', 'model.decoder.layers.4.self_attn.k_proj.weight', 'model.decoder.layers.5.encoder_attn_layer_norm.weight', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.decoder.layers.0.self_attn.k_proj.bias', 'model.decoder.layers.0.self_attn.v_proj.weight', 'model.decoder.layers.1.final_layer_norm.bias', 'model.decoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.decoder.layers.2.self_attn.q_proj.weight', 'model.decoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.decoder.layers.1.self_attn_layer_norm.weight', 'model.decoder.layers.1.final_layer_norm.weight', 'model.decoder.layers.1.encoder_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.decoder.layers.2.encoder_attn.out_proj.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.decoder.layers.5.encoder_attn_layer_norm.bias', 'model.decoder.layers.2.encoder_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.decoder.layers.0.self_attn.v_proj.bias', 'model.decoder.layers.2.self_attn.k_proj.bias', 'model.decoder.layers.3.encoder_attn.out_proj.weight', 'final_logits_bias', 'model.decoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.decoder.layers.0.encoder_attn.v_proj.weight', 'model.decoder.layers.2.self_attn.v_proj.bias', 'model.decoder.layers.3.self_attn.v_proj.weight', 'model.decoder.layers.1.fc1.bias', 'model.decoder.layers.4.encoder_attn.v_proj.weight', 'model.encoder.layers.0.final_layer_norm.bias', 'model.decoder.layers.2.self_attn.out_proj.weight', 'model.decoder.layers.2.fc2.bias', 'model.decoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.decoder.layers.2.final_layer_norm.bias', 'model.decoder.layers.1.self_attn.out_proj.weight', 'model.decoder.layers.3.self_attn.out_proj.weight', 'model.decoder.layers.3.fc1.bias', 'lm_head.weight', 'model.decoder.layers.5.encoder_attn.q_proj.bias', 'model.decoder.layers.0.self_attn.k_proj.weight', 'model.decoder.layers.4.encoder_attn.v_proj.bias', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.decoder.layers.2.encoder_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.decoder.layers.3.fc2.weight', 'model.decoder.layers.1.encoder_attn.v_proj.weight', 'model.decoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.decoder.layers.5.self_attn_layer_norm.bias', 'model.decoder.layers.4.encoder_attn.out_proj.bias', 'model.decoder.layers.1.fc1.weight', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.shared.weight', 'model.decoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.decoder.layers.1.self_attn.q_proj.bias', 'model.decoder.layers.5.encoder_attn.out_proj.weight', 'model.decoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.decoder.layers.4.encoder_attn.k_proj.weight', 'model.encoder.layers.5.fc2.weight', 'model.decoder.layers.4.final_layer_norm.weight', 'model.decoder.layers.3.encoder_attn.v_proj.weight', 'model.decoder.layers.0.self_attn_layer_norm.weight', 'model.decoder.layers.4.self_attn.q_proj.weight']\n",
      "- This IS expected if you are initializing BartForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BartForConditionalGeneration were not initialized from the model checkpoint at circulus/kobart-trans-en-ko-v2 and are newly initialized: ['model.bart.decoder.layers.1.self_attn.q_proj.bias', 'model.bart.encoder.layers.4.self_attn.out_proj.bias', 'model.bart.decoder.layers.4.self_attn.k_proj.weight', 'model.bart.decoder.layers.5.fc2.weight', 'model.bart.decoder.layers.4.final_layer_norm.bias', 'model.bart.decoder.layers.1.self_attn_layer_norm.weight', 'model.bart.decoder.layers.3.fc1.weight', 'model.bart.decoder.layers.3.self_attn.k_proj.weight', 'model.bart.decoder.layers.1.encoder_attn.q_proj.weight', 'model.bart.decoder.layers.3.self_attn.out_proj.weight', 'model.bart.encoder.layers.2.self_attn.k_proj.bias', 'model.bart.encoder.layernorm_embedding.weight', 'model.bart.decoder.layers.3.encoder_attn.out_proj.bias', 'model.bart.decoder.layers.4.encoder_attn.k_proj.weight', 'model.bart.encoder.layers.0.self_attn.k_proj.weight', 'model.bart.decoder.layers.1.self_attn.out_proj.bias', 'model.bart.encoder.layers.1.self_attn_layer_norm.weight', 'model.bart.decoder.layers.4.encoder_attn.v_proj.bias', 'model.bart.encoder.layernorm_embedding.bias', 'model.bart.decoder.layers.1.fc2.weight', 'model.bart.encoder.embed_positions.weight', 'model.bart.decoder.layers.3.encoder_attn_layer_norm.bias', 'model.bart.encoder.layers.4.self_attn_layer_norm.bias', 'model.bart.encoder.layers.3.self_attn.out_proj.weight', 'model.bart.encoder.layers.1.self_attn.v_proj.weight', 'model.bart.encoder.layers.2.self_attn.q_proj.weight', 'model.bart.encoder.layers.3.self_attn_layer_norm.bias', 'model.bart.decoder.layers.1.final_layer_norm.weight', 'model.bart.decoder.layers.3.self_attn.v_proj.weight', 'model.bart.decoder.layers.4.encoder_attn.v_proj.weight', 'model.bart.encoder.layers.4.self_attn.q_proj.weight', 'model.bart.decoder.layers.2.encoder_attn.v_proj.bias', 'model.bart.decoder.layers.5.self_attn.k_proj.bias', 'model.bart.encoder.layers.4.self_attn.k_proj.bias', 'model.bart.decoder.layers.1.self_attn_layer_norm.bias', 'model.bart.decoder.layers.0.encoder_attn.k_proj.bias', 'model.bart.decoder.layers.0.self_attn.q_proj.bias', 'model.bart.encoder.layers.2.self_attn.out_proj.bias', 'model.bart.decoder.layers.0.encoder_attn.v_proj.bias', 'model.bart.encoder.layers.0.fc2.bias', 'model.bart.decoder.layers.5.self_attn_layer_norm.bias', 'model.bart.decoder.layers.0.self_attn.v_proj.weight', 'model.bart.encoder.layers.4.self_attn.out_proj.weight', 'model.bart.decoder.layers.3.self_attn.q_proj.bias', 'model.bart.encoder.layers.0.self_attn_layer_norm.bias', 'model.bart.decoder.layers.1.self_attn.k_proj.bias', 'model.bart.encoder.layers.1.self_attn.q_proj.bias', 'model.bart.decoder.layers.5.encoder_attn.v_proj.bias', 'model.bart.decoder.layers.4.fc1.weight', 'model.bart.decoder.layers.5.encoder_attn.q_proj.weight', 'model.bart.decoder.layers.0.self_attn.k_proj.bias', 'model.bart.decoder.layers.1.final_layer_norm.bias', 'model.bart.decoder.layers.2.fc2.bias', 'model.bart.decoder.layers.3.self_attn.k_proj.bias', 'model.bart.decoder.layers.5.encoder_attn.k_proj.weight', 'model.bart.encoder.layers.3.self_attn.k_proj.weight', 'model.bart.decoder.layers.1.self_attn.v_proj.bias', 'model.bart.encoder.embed_tokens.weight', 'model.bart.decoder.layers.2.self_attn.v_proj.bias', 'model.bart.decoder.layers.1.encoder_attn.q_proj.bias', 'model.bart.decoder.layers.5.self_attn.q_proj.weight', 'model.bart.encoder.layers.2.self_attn.v_proj.bias', 'model.bart.decoder.layernorm_embedding.bias', 'model.bart.decoder.layers.0.encoder_attn.q_proj.weight', 'model.bart.decoder.layers.4.encoder_attn.out_proj.weight', 'model.bart.encoder.layers.4.final_layer_norm.weight', 'model.bart.decoder.layers.2.fc1.weight', 'model.bart.decoder.layers.3.fc2.bias', 'model.bart.decoder.layers.3.encoder_attn_layer_norm.weight', 'model.bart.decoder.layers.4.encoder_attn_layer_norm.bias', 'model.bart.encoder.layers.4.fc1.bias', 'model.bart.decoder.layers.3.self_attn.q_proj.weight', 'model.bart.decoder.layers.2.encoder_attn.k_proj.bias', 'model.bart.encoder.layers.3.final_layer_norm.bias', 'model.bart.decoder.layers.2.fc1.bias', 'model.bart.shared.weight', 'model.bart.decoder.layers.2.self_attn.k_proj.weight', 'model.bart.decoder.layers.5.self_attn.v_proj.weight', 'model.bart.encoder.layers.0.fc1.bias', 'model.bart.decoder.layers.4.encoder_attn.out_proj.bias', 'model.bart.encoder.layers.4.self_attn.v_proj.bias', 'model.bart.decoder.layers.4.encoder_attn.k_proj.bias', 'model.bart.decoder.layers.3.encoder_attn.v_proj.weight', 'model.bart.decoder.layers.4.encoder_attn.q_proj.bias', 'model.bart.encoder.layers.5.fc2.weight', 'model.bart.encoder.layers.5.self_attn.v_proj.bias', 'model.bart.encoder.layers.2.self_attn.q_proj.bias', 'model.bart.decoder.layers.1.encoder_attn.v_proj.bias', 'model.bart.decoder.layers.2.self_attn.v_proj.weight', 'model.bart.decoder.layers.3.self_attn_layer_norm.weight', 'model.bart.decoder.layers.1.fc1.weight', 'model.bart.encoder.layers.3.fc2.weight', 'model.bart.encoder.layers.5.self_attn.k_proj.bias', 'model.bart.decoder.layers.5.self_attn.q_proj.bias', 'model.bart.encoder.layers.1.fc1.bias', 'model.bart.encoder.layers.3.fc2.bias', 'model.bart.encoder.layers.2.fc2.bias', 'model.bart.decoder.layers.0.self_attn_layer_norm.bias', 'model.bart.decoder.layers.0.encoder_attn.out_proj.bias', 'model.bart.encoder.layers.4.fc2.bias', 'model.bart.decoder.layers.2.encoder_attn_layer_norm.bias', 'model.bart.decoder.layers.2.final_layer_norm.bias', 'model.bart.encoder.layers.3.final_layer_norm.weight', 'model.bart.decoder.layers.0.self_attn.k_proj.weight', 'model.bart.encoder.layers.2.self_attn.v_proj.weight', 'model.bart.encoder.layers.3.fc1.weight', 'model.bart.decoder.layers.2.self_attn.k_proj.bias', 'model.bart.decoder.layers.3.self_attn_layer_norm.bias', 'model.bart.decoder.layers.5.self_attn.out_proj.weight', 'model.lm_head.weight', 'model.bart.decoder.layers.0.encoder_attn_layer_norm.bias', 'model.bart.decoder.layers.5.encoder_attn.q_proj.bias', 'model.bart.encoder.layers.0.self_attn.q_proj.weight', 'model.bart.decoder.layers.4.self_attn_layer_norm.bias', 'model.bart.encoder.layers.0.self_attn.k_proj.bias', 'model.bart.encoder.layers.1.self_attn.out_proj.bias', 'model.bart.encoder.layers.2.final_layer_norm.weight', 'model.bart.decoder.layers.0.encoder_attn.k_proj.weight', 'model.bart.encoder.layers.0.fc2.weight', 'model.bart.decoder.layers.1.encoder_attn.v_proj.weight', 'model.bart.decoder.layers.4.self_attn.v_proj.weight', 'model.bart.encoder.layers.1.fc2.weight', 'model.bart.encoder.layers.3.self_attn.k_proj.bias', 'model.bart.decoder.layers.0.encoder_attn.v_proj.weight', 'model.bart.decoder.layers.0.encoder_attn.q_proj.bias', 'model.bart.decoder.layers.2.self_attn.q_proj.bias', 'model.bart.encoder.layers.5.final_layer_norm.weight', 'model.bart.decoder.embed_tokens.weight', 'model.bart.decoder.layers.1.fc2.bias', 'model.bart.decoder.layers.4.self_attn.v_proj.bias', 'model.bart.encoder.layers.4.final_layer_norm.bias', 'model.bart.decoder.layers.5.final_layer_norm.bias', 'model.bart.decoder.layers.4.encoder_attn.q_proj.weight', 'model.bart.encoder.layers.2.final_layer_norm.bias', 'model.bart.encoder.layers.1.fc1.weight', 'model.bart.decoder.layers.0.fc2.weight', 'model.bart.encoder.layers.0.self_attn.v_proj.weight', 'model.bart.decoder.layers.5.self_attn.out_proj.bias', 'model.bart.decoder.layers.0.self_attn.q_proj.weight', 'model.bart.encoder.layers.1.self_attn.k_proj.bias', 'model.bart.decoder.layers.0.encoder_attn.out_proj.weight', 'model.bart.decoder.layers.5.encoder_attn.k_proj.bias', 'model.bart.decoder.layers.0.self_attn.out_proj.bias', 'model.bart.decoder.layers.0.final_layer_norm.bias', 'model.bart.encoder.layers.0.self_attn.out_proj.bias', 'model.bart.decoder.layers.2.encoder_attn.out_proj.bias', 'model.bart.encoder.layers.0.final_layer_norm.bias', 'model.bart.decoder.layers.0.fc2.bias', 'model.bart.encoder.layers.5.self_attn.v_proj.weight', 'model.bart.decoder.layers.3.final_layer_norm.bias', 'model.bart.decoder.layers.4.self_attn.q_proj.weight', 'model.bart.decoder.layers.2.self_attn_layer_norm.weight', 'model.bart.decoder.layers.2.encoder_attn.v_proj.weight', 'model.bart.encoder.layers.2.self_attn.out_proj.weight', 'model.bart.decoder.layers.4.fc2.weight', 'model.bart.encoder.layers.1.self_attn.q_proj.weight', 'model.bart.decoder.layers.2.fc2.weight', 'model.bart.decoder.layers.2.final_layer_norm.weight', 'model.bart.decoder.layers.5.fc1.bias', 'model.bart.decoder.layers.3.self_attn.v_proj.bias', 'model.bart.encoder.layers.3.self_attn.q_proj.bias', 'model.bart.encoder.layers.5.fc1.bias', 'model.bart.decoder.layers.5.encoder_attn.out_proj.weight', 'model.bart.decoder.layers.5.encoder_attn_layer_norm.weight', 'model.bart.decoder.layers.2.self_attn.out_proj.weight', 'model.bart.encoder.layers.1.self_attn.v_proj.bias', 'model.bart.decoder.layers.1.encoder_attn.out_proj.bias', 'model.bart.decoder.layernorm_embedding.weight', 'model.bart.decoder.layers.4.self_attn_layer_norm.weight', 'model.bart.encoder.layers.1.self_attn.k_proj.weight', 'model.bart.decoder.layers.3.self_attn.out_proj.bias', 'model.bart.encoder.layers.5.self_attn_layer_norm.bias', 'model.bart.decoder.layers.0.fc1.weight', 'model.bart.encoder.layers.4.self_attn.q_proj.bias', 'model.bart.decoder.layers.5.fc2.bias', 'model.bart.decoder.layers.0.encoder_attn_layer_norm.weight', 'model.bart.encoder.layers.0.final_layer_norm.weight', 'model.bart.decoder.layers.5.final_layer_norm.weight', 'model.bart.encoder.layers.2.fc2.weight', 'model.bart.decoder.layers.5.self_attn_layer_norm.weight', 'model.bart.encoder.layers.3.self_attn.v_proj.bias', 'model.bart.encoder.layers.5.self_attn_layer_norm.weight', 'model.bart.decoder.layers.1.encoder_attn.k_proj.weight', 'model.bart.decoder.layers.2.encoder_attn.q_proj.bias', 'model.bart.decoder.layers.2.encoder_attn_layer_norm.weight', 'model.bart.decoder.layers.3.encoder_attn.q_proj.weight', 'model.bart.decoder.layers.3.fc1.bias', 'model.bart.encoder.layers.2.self_attn_layer_norm.bias', 'model.bart.decoder.layers.4.self_attn.out_proj.weight', 'model.bart.decoder.layers.4.fc1.bias', 'model.bart.decoder.layers.4.final_layer_norm.weight', 'model.bart.encoder.layers.1.self_attn_layer_norm.bias', 'model.bart.encoder.layers.0.self_attn.v_proj.bias', 'model.bart.encoder.layers.3.self_attn.out_proj.bias', 'model.bart.encoder.layers.2.fc1.weight', 'model.bart.encoder.layers.4.self_attn_layer_norm.weight', 'model.bart.encoder.layers.5.self_attn.k_proj.weight', 'model.bart.decoder.layers.0.self_attn_layer_norm.weight', 'model.bart.decoder.layers.3.encoder_attn.k_proj.bias', 'model.bart.decoder.layers.3.encoder_attn.q_proj.bias', 'model.bart.decoder.layers.2.encoder_attn.q_proj.weight', 'model.bart.encoder.layers.3.self_attn_layer_norm.weight', 'model.bart.encoder.layers.4.fc1.weight', 'model.bart.encoder.layers.1.fc2.bias', 'model.bart.decoder.layers.4.self_attn.q_proj.bias', 'model.bart.decoder.layers.3.final_layer_norm.weight', 'model.bart.decoder.layers.3.encoder_attn.k_proj.weight', 'model.bart.decoder.layers.1.self_attn.k_proj.weight', 'model.bart.encoder.layers.0.self_attn.out_proj.weight', 'model.bart.encoder.layers.1.final_layer_norm.weight', 'model.bart.encoder.layers.3.fc1.bias', 'model.bart.decoder.layers.1.encoder_attn_layer_norm.weight', 'model.bart.decoder.layers.3.encoder_attn.v_proj.bias', 'model.bart.decoder.layers.1.self_attn.v_proj.weight', 'model.bart.decoder.layers.4.encoder_attn_layer_norm.weight', 'model.bart.decoder.layers.2.encoder_attn.out_proj.weight', 'model.bart.decoder.layers.2.self_attn_layer_norm.bias', 'model.bart.encoder.layers.0.self_attn.q_proj.bias', 'model.bart.encoder.layers.5.self_attn.q_proj.weight', 'model.bart.decoder.layers.0.self_attn.v_proj.bias', 'model.bart.decoder.layers.1.encoder_attn.k_proj.bias', 'model.bart.decoder.layers.1.encoder_attn_layer_norm.bias', 'model.bart.encoder.layers.1.final_layer_norm.bias', 'model.bart.encoder.layers.3.self_attn.q_proj.weight', 'model.bart.encoder.layers.2.fc1.bias', 'model.bart.decoder.layers.1.encoder_attn.out_proj.weight', 'model.bart.decoder.layers.2.self_attn.q_proj.weight', 'model.bart.decoder.embed_positions.weight', 'model.bart.decoder.layers.0.final_layer_norm.weight', 'model.bart.encoder.layers.5.final_layer_norm.bias', 'model.bart.decoder.layers.0.self_attn.out_proj.weight', 'model.bart.decoder.layers.1.self_attn.out_proj.weight', 'model.bart.decoder.layers.1.fc1.bias', 'model.bart.decoder.layers.2.encoder_attn.k_proj.weight', 'model.bart.decoder.layers.3.fc2.weight', 'model.bart.decoder.layers.5.encoder_attn_layer_norm.bias', 'model.bart.decoder.layers.5.self_attn.k_proj.weight', 'model.bart.encoder.layers.3.self_attn.v_proj.weight', 'model.bart.encoder.layers.4.self_attn.v_proj.weight', 'model.bart.encoder.layers.5.self_attn.out_proj.weight', 'model.bart.encoder.layers.2.self_attn.k_proj.weight', 'model.bart.encoder.layers.0.self_attn_layer_norm.weight', 'model.bart.decoder.layers.5.encoder_attn.out_proj.bias', 'model.bart.decoder.layers.5.encoder_attn.v_proj.weight', 'model.bart.encoder.layers.5.self_attn.out_proj.bias', 'model.bart.decoder.layers.2.self_attn.out_proj.bias', 'model.bart.encoder.layers.5.self_attn.q_proj.bias', 'model.bart.decoder.layers.4.self_attn.k_proj.bias', 'model.bart.encoder.layers.1.self_attn.out_proj.weight', 'model.bart.decoder.layers.4.fc2.bias', 'model.bart.decoder.layers.5.fc1.weight', 'model.bart.encoder.layers.5.fc2.bias', 'model.bart.decoder.layers.4.self_attn.out_proj.bias', 'model.bart.decoder.layers.3.encoder_attn.out_proj.weight', 'model.bart.decoder.layers.1.self_attn.q_proj.weight', 'model.bart.decoder.layers.0.fc1.bias', 'model.bart.encoder.layers.4.fc2.weight', 'model.bart.decoder.layers.5.self_attn.v_proj.bias', 'model.bart.encoder.layers.2.self_attn_layer_norm.weight', 'model.bart.encoder.layers.0.fc1.weight', 'model.bart.encoder.layers.5.fc1.weight', 'model.bart.encoder.layers.4.self_attn.k_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp fp16 backend\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model_init=model_init, args=args,\n",
    "                  data_collator = data_collator,\n",
    "                  train_dataset=dataset_test['train'],\n",
    "                  eval_dataset=dataset_test['test'],\n",
    "                  compute_metrics=compute_metrics,\n",
    "                  tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff62ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file https://huggingface.co/circulus/kobart-trans-en-ko-v2/resolve/main/pytorch_model.bin from cache at /aiffel/.cache/huggingface/transformers/37376455ce67919ebfbfd84032875e0989f86be4686adbccec2993d3fafe29c8.0e5b5adc2067434ee15e3a7669c833bd6f2979558cfcb531c8f9e99ad40dae33\n",
      "Some weights of the model checkpoint at circulus/kobart-trans-en-ko-v2 were not used when initializing BartForConditionalGeneration: ['model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.decoder.layers.4.self_attn.v_proj.bias', 'model.decoder.layers.3.encoder_attn.k_proj.weight', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.decoder.layers.0.self_attn.out_proj.weight', 'model.decoder.layers.4.self_attn.q_proj.bias', 'model.decoder.layers.0.self_attn.q_proj.bias', 'model.decoder.layers.4.self_attn_layer_norm.bias', 'model.decoder.layers.3.fc2.bias', 'model.decoder.layers.1.fc2.weight', 'model.decoder.layers.2.self_attn.v_proj.weight', 'model.decoder.layers.2.self_attn_layer_norm.bias', 'model.decoder.layers.2.encoder_attn.k_proj.bias', 'model.decoder.layers.4.encoder_attn.q_proj.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.decoder.layers.0.encoder_attn.v_proj.bias', 'model.decoder.layers.0.final_layer_norm.weight', 'model.decoder.layers.1.encoder_attn.k_proj.weight', 'model.decoder.layers.3.fc1.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.decoder.layers.4.encoder_attn.out_proj.weight', 'model.decoder.layers.2.encoder_attn_layer_norm.bias', 'model.decoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.decoder.layers.2.self_attn.q_proj.bias', 'model.decoder.layers.3.self_attn.v_proj.bias', 'model.decoder.layers.0.fc2.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.decoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.decoder.layers.1.self_attn_layer_norm.bias', 'model.decoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.decoder.layers.3.encoder_attn_layer_norm.weight', 'model.decoder.layers.1.encoder_attn.q_proj.weight', 'model.decoder.layers.3.encoder_attn.k_proj.bias', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.decoder.layers.1.encoder_attn.k_proj.bias', 'model.decoder.layers.3.encoder_attn.q_proj.bias', 'model.decoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.decoder.layers.4.self_attn.k_proj.bias', 'model.decoder.layers.0.self_attn_layer_norm.bias', 'model.decoder.layers.5.final_layer_norm.weight', 'model.decoder.layers.2.fc1.weight', 'model.encoder.layers.1.fc1.weight', 'model.decoder.layers.0.encoder_attn.q_proj.weight', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.embed_positions.weight', 'model.decoder.layers.2.fc1.bias', 'model.encoder.layers.3.final_layer_norm.bias', 'model.decoder.layernorm_embedding.weight', 'model.decoder.layers.2.encoder_attn.v_proj.bias', 'model.decoder.layers.4.fc1.bias', 'model.decoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.decoder.layers.0.encoder_attn.out_proj.bias', 'model.decoder.layernorm_embedding.bias', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.decoder.layers.0.encoder_attn.k_proj.bias', 'model.encoder.layers.4.final_layer_norm.weight', 'model.decoder.layers.1.self_attn.k_proj.weight', 'model.decoder.layers.4.encoder_attn_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.bias', 'model.decoder.layers.2.encoder_attn.k_proj.weight', 'model.decoder.layers.4.self_attn_layer_norm.weight', 'model.decoder.layers.5.encoder_attn.v_proj.bias', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.decoder.layers.1.fc2.bias', 'model.decoder.layers.3.encoder_attn.out_proj.bias', 'model.decoder.layers.2.fc2.weight', 'model.encoder.layers.5.fc1.weight', 'model.decoder.layers.0.fc1.bias', 'model.decoder.layers.3.self_attn.k_proj.weight', 'model.decoder.layers.0.encoder_attn.q_proj.bias', 'model.encoder.layers.5.final_layer_norm.weight', 'model.decoder.layers.4.encoder_attn_layer_norm.bias', 'model.decoder.layers.5.encoder_attn.v_proj.weight', 'model.decoder.layers.0.final_layer_norm.bias', 'model.decoder.layers.1.encoder_attn.out_proj.weight', 'model.encoder.layers.2.fc2.bias', 'model.decoder.layers.3.encoder_attn_layer_norm.bias', 'model.decoder.layers.0.encoder_attn_layer_norm.weight', 'model.decoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.decoder.layers.4.encoder_attn.k_proj.bias', 'model.decoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.decoder.layers.3.self_attn.k_proj.bias', 'model.decoder.layers.5.fc1.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.4.self_attn.v_proj.bias', 'model.decoder.layers.4.encoder_attn.q_proj.bias', 'model.decoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.decoder.layers.2.encoder_attn.q_proj.bias', 'model.decoder.layers.3.encoder_attn.q_proj.weight', 'model.decoder.layers.3.self_attn_layer_norm.weight', 'model.decoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.decoder.layers.1.encoder_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.decoder.layers.0.self_attn.out_proj.bias', 'model.decoder.layers.2.self_attn.out_proj.bias', 'model.decoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.2.fc1.bias', 'model.decoder.layers.0.fc2.bias', 'model.decoder.layers.5.encoder_attn.q_proj.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.decoder.layers.4.fc2.bias', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.decoder.layers.1.self_attn.out_proj.bias', 'model.decoder.layers.5.fc2.weight', 'model.decoder.layers.0.fc1.weight', 'model.decoder.layers.5.encoder_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.decoder.layers.0.encoder_attn.k_proj.weight', 'model.decoder.layers.1.encoder_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.decoder.layers.5.encoder_attn.k_proj.weight', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.0.fc1.bias', 'model.decoder.layers.3.encoder_attn.v_proj.bias', 'model.decoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.decoder.layers.5.self_attn.out_proj.weight', 'model.decoder.layers.5.fc1.bias', 'model.decoder.embed_positions.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.embed_tokens.weight', 'model.decoder.layers.1.encoder_attn.out_proj.bias', 'model.decoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.decoder.layers.5.encoder_attn.out_proj.bias', 'model.decoder.layers.0.encoder_attn.out_proj.weight', 'model.decoder.layers.4.fc2.weight', 'model.encoder.layernorm_embedding.bias', 'model.decoder.layers.1.encoder_attn_layer_norm.bias', 'model.decoder.embed_tokens.weight', 'model.decoder.layers.3.self_attn.out_proj.bias', 'model.decoder.layers.2.encoder_attn.v_proj.weight', 'model.decoder.layers.1.self_attn.v_proj.bias', 'model.decoder.layers.3.self_attn.q_proj.weight', 'model.decoder.layers.4.fc1.weight', 'model.decoder.layers.1.self_attn.v_proj.weight', 'model.decoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.decoder.layers.5.fc2.bias', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.decoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layernorm_embedding.weight', 'model.decoder.layers.2.encoder_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.decoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.4.fc1.bias', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.0.fc2.bias', 'model.decoder.layers.0.encoder_attn_layer_norm.bias', 'model.decoder.layers.1.self_attn.q_proj.weight', 'model.decoder.layers.4.self_attn.k_proj.weight', 'model.decoder.layers.5.encoder_attn_layer_norm.weight', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.decoder.layers.0.self_attn.k_proj.bias', 'model.decoder.layers.0.self_attn.v_proj.weight', 'model.decoder.layers.1.final_layer_norm.bias', 'model.decoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.decoder.layers.2.self_attn.q_proj.weight', 'model.decoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.4.fc2.bias', 'model.decoder.layers.1.self_attn_layer_norm.weight', 'model.decoder.layers.1.final_layer_norm.weight', 'model.decoder.layers.1.encoder_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.decoder.layers.2.encoder_attn.out_proj.weight', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.decoder.layers.5.encoder_attn_layer_norm.bias', 'model.decoder.layers.2.encoder_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.decoder.layers.0.self_attn.v_proj.bias', 'model.decoder.layers.2.self_attn.k_proj.bias', 'model.decoder.layers.3.encoder_attn.out_proj.weight', 'final_logits_bias', 'model.decoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.decoder.layers.0.encoder_attn.v_proj.weight', 'model.decoder.layers.2.self_attn.v_proj.bias', 'model.decoder.layers.3.self_attn.v_proj.weight', 'model.decoder.layers.1.fc1.bias', 'model.decoder.layers.4.encoder_attn.v_proj.weight', 'model.encoder.layers.0.final_layer_norm.bias', 'model.decoder.layers.2.self_attn.out_proj.weight', 'model.decoder.layers.2.fc2.bias', 'model.decoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.decoder.layers.2.final_layer_norm.bias', 'model.decoder.layers.1.self_attn.out_proj.weight', 'model.decoder.layers.3.self_attn.out_proj.weight', 'model.decoder.layers.3.fc1.bias', 'lm_head.weight', 'model.decoder.layers.5.encoder_attn.q_proj.bias', 'model.decoder.layers.0.self_attn.k_proj.weight', 'model.decoder.layers.4.encoder_attn.v_proj.bias', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.decoder.layers.2.encoder_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.decoder.layers.3.fc2.weight', 'model.decoder.layers.1.encoder_attn.v_proj.weight', 'model.decoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.decoder.layers.5.self_attn_layer_norm.bias', 'model.decoder.layers.4.encoder_attn.out_proj.bias', 'model.decoder.layers.1.fc1.weight', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.shared.weight', 'model.decoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.decoder.layers.1.self_attn.q_proj.bias', 'model.decoder.layers.5.encoder_attn.out_proj.weight', 'model.decoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.decoder.layers.4.encoder_attn.k_proj.weight', 'model.encoder.layers.5.fc2.weight', 'model.decoder.layers.4.final_layer_norm.weight', 'model.decoder.layers.3.encoder_attn.v_proj.weight', 'model.decoder.layers.0.self_attn_layer_norm.weight', 'model.decoder.layers.4.self_attn.q_proj.weight']\n",
      "- This IS expected if you are initializing BartForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BartForConditionalGeneration were not initialized from the model checkpoint at circulus/kobart-trans-en-ko-v2 and are newly initialized: ['model.bart.decoder.layers.1.self_attn.q_proj.bias', 'model.bart.encoder.layers.4.self_attn.out_proj.bias', 'model.bart.decoder.layers.4.self_attn.k_proj.weight', 'model.bart.decoder.layers.5.fc2.weight', 'model.bart.decoder.layers.4.final_layer_norm.bias', 'model.bart.decoder.layers.1.self_attn_layer_norm.weight', 'model.bart.decoder.layers.3.fc1.weight', 'model.bart.decoder.layers.3.self_attn.k_proj.weight', 'model.bart.decoder.layers.1.encoder_attn.q_proj.weight', 'model.bart.decoder.layers.3.self_attn.out_proj.weight', 'model.bart.encoder.layers.2.self_attn.k_proj.bias', 'model.bart.encoder.layernorm_embedding.weight', 'model.bart.decoder.layers.3.encoder_attn.out_proj.bias', 'model.bart.decoder.layers.4.encoder_attn.k_proj.weight', 'model.bart.encoder.layers.0.self_attn.k_proj.weight', 'model.bart.decoder.layers.1.self_attn.out_proj.bias', 'model.bart.encoder.layers.1.self_attn_layer_norm.weight', 'model.bart.decoder.layers.4.encoder_attn.v_proj.bias', 'model.bart.encoder.layernorm_embedding.bias', 'model.bart.decoder.layers.1.fc2.weight', 'model.bart.encoder.embed_positions.weight', 'model.bart.decoder.layers.3.encoder_attn_layer_norm.bias', 'model.bart.encoder.layers.4.self_attn_layer_norm.bias', 'model.bart.encoder.layers.3.self_attn.out_proj.weight', 'model.bart.encoder.layers.1.self_attn.v_proj.weight', 'model.bart.encoder.layers.2.self_attn.q_proj.weight', 'model.bart.encoder.layers.3.self_attn_layer_norm.bias', 'model.bart.decoder.layers.1.final_layer_norm.weight', 'model.bart.decoder.layers.3.self_attn.v_proj.weight', 'model.bart.decoder.layers.4.encoder_attn.v_proj.weight', 'model.bart.encoder.layers.4.self_attn.q_proj.weight', 'model.bart.decoder.layers.2.encoder_attn.v_proj.bias', 'model.bart.decoder.layers.5.self_attn.k_proj.bias', 'model.bart.encoder.layers.4.self_attn.k_proj.bias', 'model.bart.decoder.layers.1.self_attn_layer_norm.bias', 'model.bart.decoder.layers.0.encoder_attn.k_proj.bias', 'model.bart.decoder.layers.0.self_attn.q_proj.bias', 'model.bart.encoder.layers.2.self_attn.out_proj.bias', 'model.bart.decoder.layers.0.encoder_attn.v_proj.bias', 'model.bart.encoder.layers.0.fc2.bias', 'model.bart.decoder.layers.5.self_attn_layer_norm.bias', 'model.bart.decoder.layers.0.self_attn.v_proj.weight', 'model.bart.encoder.layers.4.self_attn.out_proj.weight', 'model.bart.decoder.layers.3.self_attn.q_proj.bias', 'model.bart.encoder.layers.0.self_attn_layer_norm.bias', 'model.bart.decoder.layers.1.self_attn.k_proj.bias', 'model.bart.encoder.layers.1.self_attn.q_proj.bias', 'model.bart.decoder.layers.5.encoder_attn.v_proj.bias', 'model.bart.decoder.layers.4.fc1.weight', 'model.bart.decoder.layers.5.encoder_attn.q_proj.weight', 'model.bart.decoder.layers.0.self_attn.k_proj.bias', 'model.bart.decoder.layers.1.final_layer_norm.bias', 'model.bart.decoder.layers.2.fc2.bias', 'model.bart.decoder.layers.3.self_attn.k_proj.bias', 'model.bart.decoder.layers.5.encoder_attn.k_proj.weight', 'model.bart.encoder.layers.3.self_attn.k_proj.weight', 'model.bart.decoder.layers.1.self_attn.v_proj.bias', 'model.bart.encoder.embed_tokens.weight', 'model.bart.decoder.layers.2.self_attn.v_proj.bias', 'model.bart.decoder.layers.1.encoder_attn.q_proj.bias', 'model.bart.decoder.layers.5.self_attn.q_proj.weight', 'model.bart.encoder.layers.2.self_attn.v_proj.bias', 'model.bart.decoder.layernorm_embedding.bias', 'model.bart.decoder.layers.0.encoder_attn.q_proj.weight', 'model.bart.decoder.layers.4.encoder_attn.out_proj.weight', 'model.bart.encoder.layers.4.final_layer_norm.weight', 'model.bart.decoder.layers.2.fc1.weight', 'model.bart.decoder.layers.3.fc2.bias', 'model.bart.decoder.layers.3.encoder_attn_layer_norm.weight', 'model.bart.decoder.layers.4.encoder_attn_layer_norm.bias', 'model.bart.encoder.layers.4.fc1.bias', 'model.bart.decoder.layers.3.self_attn.q_proj.weight', 'model.bart.decoder.layers.2.encoder_attn.k_proj.bias', 'model.bart.encoder.layers.3.final_layer_norm.bias', 'model.bart.decoder.layers.2.fc1.bias', 'model.bart.shared.weight', 'model.bart.decoder.layers.2.self_attn.k_proj.weight', 'model.bart.decoder.layers.5.self_attn.v_proj.weight', 'model.bart.encoder.layers.0.fc1.bias', 'model.bart.decoder.layers.4.encoder_attn.out_proj.bias', 'model.bart.encoder.layers.4.self_attn.v_proj.bias', 'model.bart.decoder.layers.4.encoder_attn.k_proj.bias', 'model.bart.decoder.layers.3.encoder_attn.v_proj.weight', 'model.bart.decoder.layers.4.encoder_attn.q_proj.bias', 'model.bart.encoder.layers.5.fc2.weight', 'model.bart.encoder.layers.5.self_attn.v_proj.bias', 'model.bart.encoder.layers.2.self_attn.q_proj.bias', 'model.bart.decoder.layers.1.encoder_attn.v_proj.bias', 'model.bart.decoder.layers.2.self_attn.v_proj.weight', 'model.bart.decoder.layers.3.self_attn_layer_norm.weight', 'model.bart.decoder.layers.1.fc1.weight', 'model.bart.encoder.layers.3.fc2.weight', 'model.bart.encoder.layers.5.self_attn.k_proj.bias', 'model.bart.decoder.layers.5.self_attn.q_proj.bias', 'model.bart.encoder.layers.1.fc1.bias', 'model.bart.encoder.layers.3.fc2.bias', 'model.bart.encoder.layers.2.fc2.bias', 'model.bart.decoder.layers.0.self_attn_layer_norm.bias', 'model.bart.decoder.layers.0.encoder_attn.out_proj.bias', 'model.bart.encoder.layers.4.fc2.bias', 'model.bart.decoder.layers.2.encoder_attn_layer_norm.bias', 'model.bart.decoder.layers.2.final_layer_norm.bias', 'model.bart.encoder.layers.3.final_layer_norm.weight', 'model.bart.decoder.layers.0.self_attn.k_proj.weight', 'model.bart.encoder.layers.2.self_attn.v_proj.weight', 'model.bart.encoder.layers.3.fc1.weight', 'model.bart.decoder.layers.2.self_attn.k_proj.bias', 'model.bart.decoder.layers.3.self_attn_layer_norm.bias', 'model.bart.decoder.layers.5.self_attn.out_proj.weight', 'model.lm_head.weight', 'model.bart.decoder.layers.0.encoder_attn_layer_norm.bias', 'model.bart.decoder.layers.5.encoder_attn.q_proj.bias', 'model.bart.encoder.layers.0.self_attn.q_proj.weight', 'model.bart.decoder.layers.4.self_attn_layer_norm.bias', 'model.bart.encoder.layers.0.self_attn.k_proj.bias', 'model.bart.encoder.layers.1.self_attn.out_proj.bias', 'model.bart.encoder.layers.2.final_layer_norm.weight', 'model.bart.decoder.layers.0.encoder_attn.k_proj.weight', 'model.bart.encoder.layers.0.fc2.weight', 'model.bart.decoder.layers.1.encoder_attn.v_proj.weight', 'model.bart.decoder.layers.4.self_attn.v_proj.weight', 'model.bart.encoder.layers.1.fc2.weight', 'model.bart.encoder.layers.3.self_attn.k_proj.bias', 'model.bart.decoder.layers.0.encoder_attn.v_proj.weight', 'model.bart.decoder.layers.0.encoder_attn.q_proj.bias', 'model.bart.decoder.layers.2.self_attn.q_proj.bias', 'model.bart.encoder.layers.5.final_layer_norm.weight', 'model.bart.decoder.embed_tokens.weight', 'model.bart.decoder.layers.1.fc2.bias', 'model.bart.decoder.layers.4.self_attn.v_proj.bias', 'model.bart.encoder.layers.4.final_layer_norm.bias', 'model.bart.decoder.layers.5.final_layer_norm.bias', 'model.bart.decoder.layers.4.encoder_attn.q_proj.weight', 'model.bart.encoder.layers.2.final_layer_norm.bias', 'model.bart.encoder.layers.1.fc1.weight', 'model.bart.decoder.layers.0.fc2.weight', 'model.bart.encoder.layers.0.self_attn.v_proj.weight', 'model.bart.decoder.layers.5.self_attn.out_proj.bias', 'model.bart.decoder.layers.0.self_attn.q_proj.weight', 'model.bart.encoder.layers.1.self_attn.k_proj.bias', 'model.bart.decoder.layers.0.encoder_attn.out_proj.weight', 'model.bart.decoder.layers.5.encoder_attn.k_proj.bias', 'model.bart.decoder.layers.0.self_attn.out_proj.bias', 'model.bart.decoder.layers.0.final_layer_norm.bias', 'model.bart.encoder.layers.0.self_attn.out_proj.bias', 'model.bart.decoder.layers.2.encoder_attn.out_proj.bias', 'model.bart.encoder.layers.0.final_layer_norm.bias', 'model.bart.decoder.layers.0.fc2.bias', 'model.bart.encoder.layers.5.self_attn.v_proj.weight', 'model.bart.decoder.layers.3.final_layer_norm.bias', 'model.bart.decoder.layers.4.self_attn.q_proj.weight', 'model.bart.decoder.layers.2.self_attn_layer_norm.weight', 'model.bart.decoder.layers.2.encoder_attn.v_proj.weight', 'model.bart.encoder.layers.2.self_attn.out_proj.weight', 'model.bart.decoder.layers.4.fc2.weight', 'model.bart.encoder.layers.1.self_attn.q_proj.weight', 'model.bart.decoder.layers.2.fc2.weight', 'model.bart.decoder.layers.2.final_layer_norm.weight', 'model.bart.decoder.layers.5.fc1.bias', 'model.bart.decoder.layers.3.self_attn.v_proj.bias', 'model.bart.encoder.layers.3.self_attn.q_proj.bias', 'model.bart.encoder.layers.5.fc1.bias', 'model.bart.decoder.layers.5.encoder_attn.out_proj.weight', 'model.bart.decoder.layers.5.encoder_attn_layer_norm.weight', 'model.bart.decoder.layers.2.self_attn.out_proj.weight', 'model.bart.encoder.layers.1.self_attn.v_proj.bias', 'model.bart.decoder.layers.1.encoder_attn.out_proj.bias', 'model.bart.decoder.layernorm_embedding.weight', 'model.bart.decoder.layers.4.self_attn_layer_norm.weight', 'model.bart.encoder.layers.1.self_attn.k_proj.weight', 'model.bart.decoder.layers.3.self_attn.out_proj.bias', 'model.bart.encoder.layers.5.self_attn_layer_norm.bias', 'model.bart.decoder.layers.0.fc1.weight', 'model.bart.encoder.layers.4.self_attn.q_proj.bias', 'model.bart.decoder.layers.5.fc2.bias', 'model.bart.decoder.layers.0.encoder_attn_layer_norm.weight', 'model.bart.encoder.layers.0.final_layer_norm.weight', 'model.bart.decoder.layers.5.final_layer_norm.weight', 'model.bart.encoder.layers.2.fc2.weight', 'model.bart.decoder.layers.5.self_attn_layer_norm.weight', 'model.bart.encoder.layers.3.self_attn.v_proj.bias', 'model.bart.encoder.layers.5.self_attn_layer_norm.weight', 'model.bart.decoder.layers.1.encoder_attn.k_proj.weight', 'model.bart.decoder.layers.2.encoder_attn.q_proj.bias', 'model.bart.decoder.layers.2.encoder_attn_layer_norm.weight', 'model.bart.decoder.layers.3.encoder_attn.q_proj.weight', 'model.bart.decoder.layers.3.fc1.bias', 'model.bart.encoder.layers.2.self_attn_layer_norm.bias', 'model.bart.decoder.layers.4.self_attn.out_proj.weight', 'model.bart.decoder.layers.4.fc1.bias', 'model.bart.decoder.layers.4.final_layer_norm.weight', 'model.bart.encoder.layers.1.self_attn_layer_norm.bias', 'model.bart.encoder.layers.0.self_attn.v_proj.bias', 'model.bart.encoder.layers.3.self_attn.out_proj.bias', 'model.bart.encoder.layers.2.fc1.weight', 'model.bart.encoder.layers.4.self_attn_layer_norm.weight', 'model.bart.encoder.layers.5.self_attn.k_proj.weight', 'model.bart.decoder.layers.0.self_attn_layer_norm.weight', 'model.bart.decoder.layers.3.encoder_attn.k_proj.bias', 'model.bart.decoder.layers.3.encoder_attn.q_proj.bias', 'model.bart.decoder.layers.2.encoder_attn.q_proj.weight', 'model.bart.encoder.layers.3.self_attn_layer_norm.weight', 'model.bart.encoder.layers.4.fc1.weight', 'model.bart.encoder.layers.1.fc2.bias', 'model.bart.decoder.layers.4.self_attn.q_proj.bias', 'model.bart.decoder.layers.3.final_layer_norm.weight', 'model.bart.decoder.layers.3.encoder_attn.k_proj.weight', 'model.bart.decoder.layers.1.self_attn.k_proj.weight', 'model.bart.encoder.layers.0.self_attn.out_proj.weight', 'model.bart.encoder.layers.1.final_layer_norm.weight', 'model.bart.encoder.layers.3.fc1.bias', 'model.bart.decoder.layers.1.encoder_attn_layer_norm.weight', 'model.bart.decoder.layers.3.encoder_attn.v_proj.bias', 'model.bart.decoder.layers.1.self_attn.v_proj.weight', 'model.bart.decoder.layers.4.encoder_attn_layer_norm.weight', 'model.bart.decoder.layers.2.encoder_attn.out_proj.weight', 'model.bart.decoder.layers.2.self_attn_layer_norm.bias', 'model.bart.encoder.layers.0.self_attn.q_proj.bias', 'model.bart.encoder.layers.5.self_attn.q_proj.weight', 'model.bart.decoder.layers.0.self_attn.v_proj.bias', 'model.bart.decoder.layers.1.encoder_attn.k_proj.bias', 'model.bart.decoder.layers.1.encoder_attn_layer_norm.bias', 'model.bart.encoder.layers.1.final_layer_norm.bias', 'model.bart.encoder.layers.3.self_attn.q_proj.weight', 'model.bart.encoder.layers.2.fc1.bias', 'model.bart.decoder.layers.1.encoder_attn.out_proj.weight', 'model.bart.decoder.layers.2.self_attn.q_proj.weight', 'model.bart.decoder.embed_positions.weight', 'model.bart.decoder.layers.0.final_layer_norm.weight', 'model.bart.encoder.layers.5.final_layer_norm.bias', 'model.bart.decoder.layers.0.self_attn.out_proj.weight', 'model.bart.decoder.layers.1.self_attn.out_proj.weight', 'model.bart.decoder.layers.1.fc1.bias', 'model.bart.decoder.layers.2.encoder_attn.k_proj.weight', 'model.bart.decoder.layers.3.fc2.weight', 'model.bart.decoder.layers.5.encoder_attn_layer_norm.bias', 'model.bart.decoder.layers.5.self_attn.k_proj.weight', 'model.bart.encoder.layers.3.self_attn.v_proj.weight', 'model.bart.encoder.layers.4.self_attn.v_proj.weight', 'model.bart.encoder.layers.5.self_attn.out_proj.weight', 'model.bart.encoder.layers.2.self_attn.k_proj.weight', 'model.bart.encoder.layers.0.self_attn_layer_norm.weight', 'model.bart.decoder.layers.5.encoder_attn.out_proj.bias', 'model.bart.decoder.layers.5.encoder_attn.v_proj.weight', 'model.bart.encoder.layers.5.self_attn.out_proj.bias', 'model.bart.decoder.layers.2.self_attn.out_proj.bias', 'model.bart.encoder.layers.5.self_attn.q_proj.bias', 'model.bart.decoder.layers.4.self_attn.k_proj.bias', 'model.bart.encoder.layers.1.self_attn.out_proj.weight', 'model.bart.decoder.layers.4.fc2.bias', 'model.bart.decoder.layers.5.fc1.weight', 'model.bart.encoder.layers.5.fc2.bias', 'model.bart.decoder.layers.4.self_attn.out_proj.bias', 'model.bart.decoder.layers.3.encoder_attn.out_proj.weight', 'model.bart.decoder.layers.1.self_attn.q_proj.weight', 'model.bart.decoder.layers.0.fc1.bias', 'model.bart.encoder.layers.4.fc2.weight', 'model.bart.decoder.layers.5.self_attn.v_proj.bias', 'model.bart.encoder.layers.2.self_attn_layer_norm.weight', 'model.bart.encoder.layers.0.fc1.weight', 'model.bart.encoder.layers.5.fc1.weight', 'model.bart.encoder.layers.4.self_attn.k_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 1001\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 501\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f15fb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5ef309",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'][:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
