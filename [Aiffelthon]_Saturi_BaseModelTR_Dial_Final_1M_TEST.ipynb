{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90d9f3a2",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Step-1.-데이터-다운로드\" data-toc-modified-id=\"Step-1.-데이터-다운로드-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Step 1. 데이터 다운로드</a></span></li><li><span><a href=\"#Step-2.-데이터-정제-및-토큰화\" data-toc-modified-id=\"Step-2.-데이터-정제-및-토큰화-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Step 2. 데이터 정제 및 토큰화</a></span></li><li><span><a href=\"#Step-3.-모델설계\" data-toc-modified-id=\"Step-3.-모델설계-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Step 3. 모델설계</a></span><ul class=\"toc-item\"><li><span><a href=\"#Positional-Encoding\" data-toc-modified-id=\"Positional-Encoding-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Positional Encoding</a></span></li><li><span><a href=\"#Multi-Head-Attention\" data-toc-modified-id=\"Multi-Head-Attention-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Multi-Head Attention</a></span></li><li><span><a href=\"#Position-wise-Feed-Forward-Network\" data-toc-modified-id=\"Position-wise-Feed-Forward-Network-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Position-wise Feed-Forward Network</a></span></li><li><span><a href=\"#Encoder-레이어-구현하기\" data-toc-modified-id=\"Encoder-레이어-구현하기-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Encoder 레이어 구현하기</a></span></li><li><span><a href=\"#Decoder-레이어-구현하기\" data-toc-modified-id=\"Decoder-레이어-구현하기-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Decoder 레이어 구현하기</a></span></li><li><span><a href=\"#Encoder와-Decoder-클래스를-정의\" data-toc-modified-id=\"Encoder와-Decoder-클래스를-정의-3.6\"><span class=\"toc-item-num\">3.6&nbsp;&nbsp;</span>Encoder와 Decoder 클래스를 정의</a></span></li><li><span><a href=\"#Transformer-완성하기\" data-toc-modified-id=\"Transformer-완성하기-3.7\"><span class=\"toc-item-num\">3.7&nbsp;&nbsp;</span>Transformer 완성하기</a></span></li><li><span><a href=\"#Mask\" data-toc-modified-id=\"Mask-3.8\"><span class=\"toc-item-num\">3.8&nbsp;&nbsp;</span>Mask</a></span></li></ul></li><li><span><a href=\"#Step-4.-훈련하기\" data-toc-modified-id=\"Step-4.-훈련하기-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Step 4. 훈련하기</a></span><ul class=\"toc-item\"><li><span><a href=\"#Learning-Rate-Scheduler를-선언하고,-이를-포함하는-Adam-Optimizer를-선언\" data-toc-modified-id=\"Learning-Rate-Scheduler를-선언하고,-이를-포함하는-Adam-Optimizer를-선언-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Learning Rate Scheduler를 선언하고, 이를 포함하는 Adam Optimizer를 선언</a></span></li><li><span><a href=\"#Loss-함수를-정의\" data-toc-modified-id=\"Loss-함수를-정의-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Loss 함수를 정의</a></span></li><li><span><a href=\"#train_step-함수를-정의\" data-toc-modified-id=\"train_step-함수를-정의-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>train_step 함수를 정의</a></span></li><li><span><a href=\"#Attention-시각화-함수\" data-toc-modified-id=\"Attention-시각화-함수-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Attention 시각화 함수</a></span></li><li><span><a href=\"#번역생성함수\" data-toc-modified-id=\"번역생성함수-4.5\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;</span>번역생성함수</a></span></li><li><span><a href=\"#학습\" data-toc-modified-id=\"학습-4.6\"><span class=\"toc-item-num\">4.6&nbsp;&nbsp;</span>학습</a></span></li></ul></li><li><span><a href=\"#Step-5.-평가\" data-toc-modified-id=\"Step-5.-평가-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Step 5. 평가</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92eff5f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in /opt/conda/lib/python3.9/site-packages (0.13.11)\n",
      "Requirement already satisfied: setproctitle in /opt/conda/lib/python3.9/site-packages (from wandb) (1.3.2)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (3.1.31)\n",
      "Requirement already satisfied: pathtools in /opt/conda/lib/python3.9/site-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from wandb) (4.0.1)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (1.16.0)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.9/site-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (3.19.1)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (2.26.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from wandb) (59.4.0)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (8.0.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (5.8.0)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.9/site-packages (from wandb) (6.0)\n",
      "Requirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.9/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.9/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2.0.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# # !pip3 list\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e81efa24",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wandb.keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31/2803453377.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWandbCallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wandb.keras'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "\n",
    "\n",
    "# from tqdm import tqdm_notebook \n",
    "from tqdm.notebook import tqdm \n",
    "import random\n",
    "\n",
    "import sentencepiece as spm\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "print(tf.__version__)\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63cee36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b56c80c",
   "metadata": {},
   "source": [
    "# Step 1. 데이터 다운로드\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f32c663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data load\n",
    "data_dir = os.getenv('HOME')+'/aiffel/aiffelthon/data'\n",
    "data_train_path = data_dir+\"/train_v_final_0310.csv\"\n",
    "data_test_path = data_dir+\"/test_v_final_0310.csv\"\n",
    "full_data = pd.read_csv(data_train_path)\n",
    "full_data_test = pd.read_csv(data_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0135dc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3ada2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #for test\n",
    "# full_data = full_data.sample(frac=1).reset_index(drop=True)\n",
    "# full_data = full_data[:10000]\n",
    "# full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddf0d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee766c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #for test\n",
    "# full_data_test = full_data_test.sample(frac=1).reset_index(drop=True)\n",
    "# full_data_test = full_data_test[:500]\n",
    "# full_data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339bdedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_data=pd.concat([full_data, full_data_test], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864aacbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data.info() #full_data: 1065918 full_data_test: 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb93860",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb35065",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a2fa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c76a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_null_dupl(data):\n",
    "    data=data.dropna(axis=0)\n",
    "    data=data[~data.duplicated(subset=['reg','topic','eng','dial'], keep= 'first')]\n",
    "    data=data.reset_index(drop=True)# index reset하기 - 기존 index 제거 O\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52857a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data=del_null_dupl(full_data) # ['text', 'dial', 'reg', 'eng'])\n",
    "full_data_test=del_null_dupl(full_data_test) # ['text', 'dial', 'reg', 'eng'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5090cde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6c5848",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data['eng_length'] = full_data['eng'].apply(lambda x : len(str(x).split()))\n",
    "full_data['dial_length'] = full_data['dial'].apply(lambda x : len(str(x).split()))\n",
    "full_data['eng_c_length'] = full_data['eng'].apply(lambda x : len(str(x)))\n",
    "full_data['dial_c_length'] = full_data['dial'].apply(lambda x : len(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5156741",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_test['eng_length'] = full_data_test['eng'].apply(lambda x : len(str(x).split()))\n",
    "full_data_test['dial_length'] = full_data_test['dial'].apply(lambda x : len(str(x).split()))\n",
    "full_data_test['eng_c_length'] = full_data_test['eng'].apply(lambda x : len(str(x)))\n",
    "full_data_test['dial_c_length'] = full_data_test['dial'].apply(lambda x : len(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6792da78",
   "metadata": {},
   "outputs": [],
   "source": [
    "regs = ['jj', 'jd', 'gs', 'cc', 'kw']\n",
    "\n",
    "for reg in regs:\n",
    "    count = full_data['dial'][full_data['reg']==reg].count()\n",
    "    print(f'Dial_{reg} :', count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc1e0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data.groupby(full_data['reg'])['eng_length'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e801c9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data.groupby(full_data['reg'])['dial_length'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfef4325",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data.groupby(full_data['reg'])['eng_length','dial_length'].agg(['max','min'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2c7ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data[\"reg_eng\"] = \"<\" + full_data[\"reg\"] +\"> \"+full_data[\"eng\"]\n",
    "full_data[\"reg_eng\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4d394f",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_test[\"reg_eng\"] = \"<\" + full_data_test[\"reg\"] +\"> \"+full_data_test[\"eng\"]\n",
    "full_data_test[\"reg_eng\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9430a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a4e1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ce8804",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(full_data['dial'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b920f5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. 모든 입력을 소문자로 변환합니다.\n",
    "# # 2. 알파벳, 문장부호, 한글만 남기고 모두 제거합니다.\n",
    "# # 3. 문장부호 양옆에 공백을 추가합니다.\n",
    "# # 4. 문장 앞뒤의 불필요한 공백을 제거합니다.\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "\n",
    "    length = len(sentence)\n",
    "    match = re.search(r'(.+)\\1{3,}', sentence) \n",
    "    if match or length == 0:\n",
    "        return None\n",
    "\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!<>가-힣ㄱ-ㅎㅏ-ㅣ]+\", \" \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c838021",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data['eng_length'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3d23fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data['eng_c_length'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ef620f",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = full_data[full_data['eng_c_length'] < 500]\n",
    "full_data_test = full_data_test[full_data_test['eng_c_length'] < 500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93da58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder corpus\n",
    "enc_corpus = full_data['reg_eng'].apply(lambda x : preprocess_sentence(x)).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100c120a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder test corpus\n",
    "enc_corpus_test = full_data_test['reg_eng'].apply(lambda x : preprocess_sentence(x)).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e356a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 지역별 평가를 위해 분리\n",
    "# 지역별 encoder test corpus \n",
    "enc_corpus_test_jj = full_data_test['reg_eng'][full_data_test['reg']=='jj'].apply(lambda x : preprocess_sentence(x)).sort_index()\n",
    "enc_corpus_test_jd = full_data_test['reg_eng'][full_data_test['reg']=='jd'].apply(lambda x : preprocess_sentence(x)).sort_index()\n",
    "enc_corpus_test_gs = full_data_test['reg_eng'][full_data_test['reg']=='gs'].apply(lambda x : preprocess_sentence(x)).sort_index()\n",
    "enc_corpus_test_cc = full_data_test['reg_eng'][full_data_test['reg']=='cc'].apply(lambda x : preprocess_sentence(x)).sort_index()\n",
    "enc_corpus_test_kw = full_data_test['reg_eng'][full_data_test['reg']=='kw'].apply(lambda x : preprocess_sentence(x)).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08166c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder corpus\n",
    "dec_corpus = full_data['dial'].apply(lambda x : preprocess_sentence(x)).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968253a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder corpus test\n",
    "dec_corpus_test = full_data_test['dial'].apply(lambda x : preprocess_sentence(x)).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ac7f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 지역별 평가를 위해 분리\n",
    "# 지역별 decoder test corpus \n",
    "dec_corpus_test_jj = full_data_test['dial'][full_data_test['reg']=='jj'].apply(lambda x : preprocess_sentence(x)).sort_index()\n",
    "dec_corpus_test_jd = full_data_test['dial'][full_data_test['reg']=='jd'].apply(lambda x : preprocess_sentence(x)).sort_index()\n",
    "dec_corpus_test_gs = full_data_test['dial'][full_data_test['reg']=='gs'].apply(lambda x : preprocess_sentence(x)).sort_index()\n",
    "dec_corpus_test_cc = full_data_test['dial'][full_data_test['reg']=='cc'].apply(lambda x : preprocess_sentence(x)).sort_index()\n",
    "dec_corpus_test_kw = full_data_test['dial'][full_data_test['reg']=='kw'].apply(lambda x : preprocess_sentence(x)).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0d105b",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_corpus_test_jj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144462a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_corpus_test_jj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202e84a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enc_corpus 통계\n",
    "\n",
    "min_len = 999\n",
    "max_len = 0\n",
    "sum_len = 0\n",
    "\n",
    "\n",
    "for sen in enc_corpus:\n",
    "    length = len(str(sen))\n",
    "    if min_len > length: min_len = length\n",
    "    if max_len < length: max_len = length\n",
    "    sum_len += length\n",
    "\n",
    "print(\"문장의 최단 길이:\", min_len)\n",
    "print(\"문장의 최장 길이:\", max_len)\n",
    "print(\"문장의 평균 길이:\", sum_len // len(enc_corpus))\n",
    "\n",
    "sentence_length = np.zeros((max_len), dtype=int)\n",
    "\n",
    "#총 max_len의 배열을 만든 후, raw 문장을 돌면서 각 문장별 길이를 sentence_length의 len(sen) 인덱스마다  계속 더해가면서 counting\n",
    "for sen in enc_corpus:\n",
    "    sentence_length[len(str(sen))-1] += 1\n",
    "\n",
    "plt.bar(range(max_len), sentence_length, width=1.0)\n",
    "plt.title(\"Sentence Length Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dc95b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(enc_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d91ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_data = full_data[full_data['eng_c_length']<400]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c3fb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dec_corpus_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec864d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dec_corpus 통계\n",
    "\n",
    "min_len = 999\n",
    "max_len = 0\n",
    "sum_len = 0\n",
    "\n",
    "\n",
    "for sen in dec_corpus:\n",
    "    length = len(str(sen))\n",
    "    if min_len > length: min_len = length\n",
    "    if max_len < length: max_len = length\n",
    "    sum_len += length\n",
    "\n",
    "print(\"문장의 최단 길이:\", min_len)\n",
    "print(\"문장의 최장 길이:\", max_len)\n",
    "print(\"문장의 평균 길이:\", sum_len // len(dec_corpus))\n",
    "\n",
    "sentence_length = np.zeros((max_len), dtype=int)\n",
    "\n",
    "#총 max_len의 배열을 만든 후, raw 문장을 돌면서 각 문장별 길이를 sentence_length의 len(sen) 인덱스마다  계속 더해가면서 counting\n",
    "for sen in dec_corpus:\n",
    "    sentence_length[len(str(sen))-1] += 1\n",
    "\n",
    "plt.bar(range(max_len), sentence_length, width=1.0)\n",
    "plt.title(\"Sentence Length Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4687e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(enc_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec21e199",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dec_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f072dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #훈련 및 테스트셋 분리\n",
    "# enc_corpus=enc_corpus[:-100]\n",
    "# enc_corpus_test = enc_corpus[-100:]\n",
    "# dec_corpus=dec_corpus[:-100]\n",
    "# dec_corpus_test = dec_corpus[-100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b1869f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(enc_corpus.shape)\n",
    "# print(enc_corpus_test.shape)\n",
    "# print(dec_corpus.shape)\n",
    "# print(dec_corpus_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845c4bac",
   "metadata": {},
   "source": [
    "# Step 2. 데이터 정제 및 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c07b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train 데이터 정제 및 토큰화\n",
    "def clean_corpus(enc_corpus, dec_corpus):\n",
    "\n",
    "    assert len(enc_corpus) == len(dec_corpus) # enc_corpus, dec_corpus가 같은 갯수라는 것을 검증받기 위해 적용\n",
    "\n",
    "    cleaned_corpus = list(set(zip(enc_corpus, dec_corpus)))  # 중복된 데이터 제거\n",
    "    \n",
    "    return cleaned_corpus\n",
    "\n",
    "cleaned_corpus = clean_corpus(enc_corpus, dec_corpus)\n",
    "len(cleaned_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b062dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39418e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <PAD> : 0 / <BOS> : 1 / <EOS> : 2 / <UNK> : 3\n",
    "# <jj> : 4 / <jd> : 5 / <gs> : 6 / <cc> : 7 / <kw> : 8\n",
    "\n",
    "# Sentencepiece를 활용하여 학습한 tokenizer를 생성\n",
    "def generate_tokenizer(corpus, vocab_size, lang=\"en\", pad_id=0, bos_id=1, eos_id=2, unk_id=3):\n",
    "\n",
    "    temp_file = os.getenv('HOME') + f'/aiffel/aiffelthon/corpus_{lang}_r0.txt'     # corpus를 받아 txt파일로 저장\n",
    "    \n",
    "    with open(temp_file, 'w') as f:\n",
    "        for row in corpus:\n",
    "            f.write(str(row) + '\\n')\n",
    "    \n",
    "    # Sentencepiece를 이용해 \n",
    "    spm.SentencePieceTrainer.Train(\n",
    "        f'--input={temp_file} --model_type=bpe --pad_id={pad_id} --bos_id={bos_id} --eos_id={eos_id} \\\n",
    "        --unk_id={unk_id} --model_prefix=spm{lang}_test_r0 --vocab_size={vocab_size} \\\n",
    "        --user_defined_symbols=<jj>,<jd>,<gs>,<cc>,<kw> --remove_extra_whitespaces=false'   # model_r0\n",
    "    )\n",
    "    tokenizer = spm.SentencePieceProcessor()\n",
    "    tokenizer.Load(f'spm{lang}_test_r0.model') # model_r0\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713cd9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_VOCAB_SIZE = TGT_VOCAB_SIZE = 4009\n",
    "\n",
    "enc_corpus = []\n",
    "dec_corpus = []\n",
    "\n",
    "for pair in cleaned_corpus:\n",
    "    e, d = pair[0], pair[1]\n",
    "    # enc, dec 분리\n",
    "    enc_corpus.append(e)\n",
    "    dec_corpus.append(d)\n",
    "\n",
    "#     enc_corpus.append(preprocess_sentence(e))\n",
    "#     dec_corpus.append(preprocess_sentence(d))\n",
    "\n",
    "enc_tokenizer = generate_tokenizer(enc_corpus, SRC_VOCAB_SIZE, \"enc\")\n",
    "dec_tokenizer = generate_tokenizer(dec_corpus, TGT_VOCAB_SIZE, \"dec\")\n",
    "dec_tokenizer.set_encode_extra_options(\"bos:eos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed960bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7747ad84",
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d55b2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(enc_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f76b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dec_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ffb08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(enc_tokenizer.EncodeAsPieces(enc_corpus[200]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822b8521",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dec_tokenizer.EncodeAsPieces(dec_corpus[200]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ce452e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(enc_tokenizer.EncodeAsIds(enc_corpus[900]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79eb763e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dec_tokenizer.EncodeAsIds(dec_corpus[900]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0a3360",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dec_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7410f2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 토크나이저를 활용해 토큰의 길이가 >5 이상인 데이터를 선별하여 src_corpus 와 tgt_corpus 를 각각 구축하고, 텐서 enc_train 과 dec_train 으로 변환\n",
    "src_corpus = [] #영어\n",
    "tgt_corpus = [] #사투리\n",
    "\n",
    "assert len(enc_corpus) == len(dec_corpus)\n",
    "\n",
    "# 토큰의 길이가 xxx 이하인 문장만 남깁니다. \n",
    "for idx in tqdm(range(len(enc_corpus))):\n",
    "    src = enc_tokenizer.EncodeAsIds(str(enc_corpus[idx]))\n",
    "    tgt = dec_tokenizer.EncodeAsIds(str(dec_corpus[idx]))\n",
    "    \n",
    "    if len(src) >= 1: \n",
    "        src_corpus.append(src)\n",
    "        tgt_corpus.append(tgt)\n",
    "\n",
    "# 패딩처리를 완료하여 학습용 데이터를 완성합니다. \n",
    "enc_train = tf.keras.preprocessing.sequence.pad_sequences(src_corpus, padding='post')\n",
    "dec_train = tf.keras.preprocessing.sequence.pad_sequences(tgt_corpus, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3816d5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 훈련 데이터와 검증 데이터로 분리하기\n",
    "# enc_train, enc_val, dec_train, dec_val = train_test_split(enc_data, dec_data, test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aca30a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# enc, dec 의 seq_length는 달라도 상관없음.\n",
    "print(enc_train.shape)\n",
    "print(dec_train.shape)\n",
    "# print(enc_val.shape)\n",
    "# print(dec_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b59f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_tokenizer.encode('<jd>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd42bb0",
   "metadata": {},
   "source": [
    "# Step 3. 모델설계"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d5a663",
   "metadata": {},
   "source": [
    "## Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea057b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos - 단어가 위치한 Time-step(각각의 토큰의 위치정보값이며 정수값을 의미)\n",
    "# d_model - 모델의 Embedding 차원 수\n",
    "# i - Encoding차원의 index\n",
    "\n",
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, int(i)/d_model)  # np.power(a,b) > a^b(제곱)\n",
    "    \n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "    \n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "    \n",
    "    # 배열의 짝수 인덱스(2i)에는 사인 함수 적용\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    # 배열의 홀수 인덱스(2i+1)에는 코사인 함수 적용\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "    \n",
    "    return sinusoid_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309c2a0c",
   "metadata": {},
   "source": [
    "## Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5728d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        self.W_q = tf.keras.layers.Dense(d_model)  # Linear Layer\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
    "        \n",
    "        # Scaled QK 값 구하기\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)\n",
    "        scaled_qk = QK / tf.math.sqrt(d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scaled_qk += (mask * -1e9)\n",
    "        \n",
    "        # 1. Attention Weights 값 구하기 -> attentions\n",
    "        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n",
    "        # 2. Attention 값을 V에 곱하기 -> out\n",
    "        out = tf.matmul(attentions, V)\n",
    "        return out, attentions\n",
    "    \n",
    "    def split_heads(self, x):\n",
    "        \"\"\"\n",
    "        Embedding된 입력을 head의 수로 분할하는 함수\n",
    "        \n",
    "        x: [ batch x length x emb ]\n",
    "        return: [ batch x length x heads x self.depth ]\n",
    "        \"\"\"\n",
    "        bsz = x.shape[0]\n",
    "        split_x = tf.reshape(x, (bsz, -1, self.num_heads, self.depth))\n",
    "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
    "        return split_x\n",
    "    \n",
    "    def combine_heads(self, x):\n",
    "        \"\"\"\n",
    "        분할된 Embedding을 하나로 결합하는 함수\n",
    "        \n",
    "        x: [ batch x length x heads x self.depth ]\n",
    "        return: [ batch x length x emb ]\n",
    "        \"\"\"\n",
    "        bsz = x.shape[0]\n",
    "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(combined_x, (bsz, -1, self.d_model))\n",
    "        return combined_x\n",
    "    \n",
    "    def call(self, Q, K, V, mask):\n",
    "        \"\"\"\n",
    "        Step 1: Linear_in(Q, K, V) -> WQ, WK, WV\n",
    "        Step 2: Split Heads(WQ, WK, WV) -> WQ_split, WK_split, WV_split\n",
    "        Step 3: Scaled Dot Product Attention(WQ_split, WK_split, WV_split)\n",
    "                 -> out, attention_weights\n",
    "        Step 4: Combine Heads(out) -> out\n",
    "        Step 5: Linear_out(out) -> out\n",
    "        \"\"\"\n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "        \n",
    "        WQ_splits = self.split_heads(WQ)\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "        \n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "            WQ_splits, WK_splits, WV_splits, mask\n",
    "        )\n",
    "        \n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b6edd9",
   "metadata": {},
   "source": [
    "## Position-wise Feed-Forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42733d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.w_1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.w_2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.w_1(x)\n",
    "        out = self.w_2(out)\n",
    "            \n",
    "        return out\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b1e539",
   "metadata": {},
   "source": [
    "## Encoder 레이어 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9068f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "        \n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        # Multi-Head Attention\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.do(out)\n",
    "        out += residual*1 \n",
    "        \n",
    "        # Position-Wise Feed Forward Network\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual*1 \n",
    "        \n",
    "        return out, enc_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c68c53",
   "metadata": {},
   "source": [
    "## Decoder 레이어 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89323017",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "        # Masked Multi-Head Attention\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        #out, dec_attn = self.dec_self_attn(out, out, out, causality_mask)\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, padding_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        # Multi-Head Attention\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        #out, dec_enc_attn = self.enc_dec_attn(out, enc_out, enc_out, padding_mask)\n",
    "        out, dec_enc_attn = self.enc_dec_attn(out, enc_out, enc_out, causality_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        # Position-Wise Feed Forward Network\n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8081ff3f",
   "metadata": {},
   "source": [
    "## Encoder와 Decoder 클래스를 정의\n",
    "EncodeLayer 와 DecoderLayer 를 모두 정의했으니 이를 조립하는 것은 어렵지 않겠죠? 이를 이용해 Encoder와 Decoder 클래스를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7631939",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, n_layers, d_model, n_heads, d_ff, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)]\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        out = x\n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "            \n",
    "        return out, enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032c1c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, n_layers, d_model, n_heads, d_ff, dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)]\n",
    "        \n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "        out = x\n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = self.dec_layers[i](out, enc_out, causality_mask, padding_mask)\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "        \n",
    "        return out, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710efd24",
   "metadata": {},
   "source": [
    "## Transformer 완성하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a976e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, n_layers, d_model, n_heads, d_ff, src_vocab_size, tgt_vocab_size,\n",
    "                 pos_len, dropout=0.2, shared=True):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        \n",
    "        # 1. Embedding Layer 정의\n",
    "        self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "        \n",
    "        # 2. Positional Encoding 정의\n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        # 6. Dropout 정의\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "        # 3. Encoder / Decoder 정의\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        \n",
    "        # 4. Output Linear 정의\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "        \n",
    "        # 5. Shared Weights\n",
    "        self.shared = shared\n",
    "        \n",
    "        if shared:\n",
    "            self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "        \n",
    "        \n",
    "    def embedding(self, emb, x):\n",
    "        \"\"\"\n",
    "        입력된 정수 배열을 Embedding + Pos Encoding\n",
    "        + Shared일 경우 Scaling 작업 포함\n",
    "\n",
    "        x: [ batch x length ]\n",
    "        return: [ batch x length x emb ]\n",
    "        \"\"\"\n",
    "        seq_len = x.shape[1]\n",
    "        out = emb(x)\n",
    "        \n",
    "        if self.shared:\n",
    "            out *= tf.math.sqrt(self.d_model)\n",
    "        \n",
    "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
    "        out = self.do(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def call(self, enc_in, dec_in, enc_mask, causality_mask, dec_mask):\n",
    "        # Step 1: Embedding(enc_in, dec_in) -> enc_in, dec_in\n",
    "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
    "        \n",
    "        # Step 2: Encoder(enc_in, enc_mask) -> enc_out, enc_attns\n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "\n",
    "        # Step 3: Decoder(dec_in, enc_out, mask) -> dec_out, dec_attns, dec_enc_attns\n",
    "        dec_out, dec_attns, dec_enc_attns = self.decoder(dec_in, enc_out, causality_mask, dec_mask)\n",
    "        \n",
    "        # Step 4: Out Linear(dec_out) -> logits\n",
    "        logits = self.fc(dec_out)\n",
    "        \n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390da50a",
   "metadata": {},
   "source": [
    "## Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce025efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention을 할 때에 <PAD> 토큰에도 Attention을 주는 것을 방지해 주는 역할\n",
    "def generate_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def generate_causality_mask(src_len, tgt_len):\n",
    "    mask = 1 - np.cumsum(np.eye(src_len, tgt_len), 0)\n",
    "    return tf.cast(mask, tf.float32)\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    dec_mask = generate_padding_mask(tgt)\n",
    "\n",
    "    dec_enc_causality_mask = generate_causality_mask(tgt.shape[1], src.shape[1])\n",
    "    dec_enc_mask = tf.maximum(enc_mask, dec_enc_causality_mask)\n",
    "\n",
    "    dec_causality_mask = generate_causality_mask(tgt.shape[1], tgt.shape[1])\n",
    "    dec_mask = tf.maximum(dec_mask, dec_causality_mask)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c0df69",
   "metadata": {},
   "source": [
    "# Step 4. 훈련하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc16c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"saturi\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "\n",
    "        \"n_layers\": 2,\n",
    "        \"d_model\":512,\n",
    "        \"n_heads\":8,\n",
    "        \"d_ff\":2048,\n",
    "        \"src_vocab_size\":4000,\n",
    "        \"tgt_vocab_size\":4000,\n",
    "        \"pos_len\":200,\n",
    "        \"dropout\":0.2,\n",
    "        \"epochs\":3,\n",
    "        \"BATCH_SIZE\":64,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712be514",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    n_layers=2,\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    d_ff=2048,\n",
    "    src_vocab_size=SRC_VOCAB_SIZE,\n",
    "    tgt_vocab_size=TGT_VOCAB_SIZE,\n",
    "    pos_len=200,\n",
    "    dropout=0.2,\n",
    "    shared=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a98781",
   "metadata": {},
   "source": [
    "##  Learning Rate Scheduler를 선언하고, 이를 포함하는 Adam Optimizer를 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb0b423",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        \n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc537286",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = LearningRateScheduler(512)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6775297",
   "metadata": {},
   "source": [
    "## Loss 함수를 정의\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4915c911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss 함수 정의\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    # Masking 되지 않은 입력의 개수로 Scaling하는 과정\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d00beae",
   "metadata": {},
   "source": [
    "## train_step 함수를 정의\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc4ee82",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    gold = tgt[:, 1:]\n",
    "        \n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt)\n",
    "\n",
    "    # 미분을 위한 GradientTape 적용\n",
    "    with tf.GradientTape() as tape:\n",
    "        # 1. 예측\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = model(src, tgt, enc_mask, dec_enc_mask, dec_mask)\n",
    "        # 2. Loss 계산\n",
    "        loss = loss_function(gold, predictions[:, :-1])\n",
    "\n",
    "     \n",
    "\n",
    "    # 3. gradient 계산\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)    \n",
    "    # 4. 오차역전파(Backpropagation) - weight 업데이트\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    # wandb log \n",
    "    # wandb.log({\"loss\": loss})\n",
    "    \n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a77246",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = os.getenv('HOME') + '/aiffel/aiffelthon/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0aeab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(os.getcwd())\n",
    "# os.listdir(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e3b459",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = tf.train.Checkpoint(step = tf.Variable(1), optimizer = optimizer , transformer = transformer)\n",
    "manager = tf.train.CheckpointManager(ckpt, directory +'tf_ckpts_gd12_test_r0',max_to_keep=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6847c0cc",
   "metadata": {},
   "source": [
    "## Attention 시각화 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ed87bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention 시각화 함수\n",
    "def visualize_attention(src, tgt, enc_attns, dec_attns, dec_enc_attns):\n",
    "    def draw(data, ax, x=\"auto\", y=\"auto\"):\n",
    "        import seaborn\n",
    "        seaborn.heatmap(data, \n",
    "                        square=True,\n",
    "                        vmin=0.0, vmax=1.0, \n",
    "                        cbar=False, ax=ax,\n",
    "                        xticklabels=x,\n",
    "                        yticklabels=y)\n",
    "        \n",
    "    for layer in range(0, 2, 1):\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        print(\"Encoder Layer\", layer + 1)\n",
    "        for h in range(4):\n",
    "            draw(enc_attns[layer][0, h, :len(src), :len(src)], axs[h], src, src)\n",
    "        plt.show()\n",
    "        \n",
    "    for layer in range(0, 2, 1):\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        print(\"Decoder Self Layer\", layer+1)\n",
    "        for h in range(4):\n",
    "            draw(dec_attns[layer][0, h, :len(tgt), :len(tgt)], axs[h], tgt, tgt)\n",
    "        plt.show()\n",
    "\n",
    "        print(\"Decoder Src Layer\", layer+1)\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        for h in range(4):\n",
    "            draw(dec_enc_attns[layer][0, h, :len(tgt), :len(src)], axs[h], src, tgt)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832f5ed3",
   "metadata": {},
   "source": [
    "## 번역생성함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affbbd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 번역 생성 함수\n",
    "def evaluate(sentence, model, reg, src_tokenizer, tgt_tokenizer):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    pieces = src_tokenizer.encode_as_pieces(sentence)\n",
    "    tokens = src_tokenizer.encode_as_ids(sentence)\n",
    "\n",
    "    _input = tf.keras.preprocessing.sequence.pad_sequences([tokens], maxlen=enc_train.shape[-1], padding='post')\n",
    "\n",
    "    ids = []\n",
    "\n",
    "    #     output = tf.expand_dims([tgt_tokenizer.bos_id()], 0)    \n",
    "    # jj > 4, jd > 5, gs > 6 , cc > 7 , kw > 8\n",
    "    output = tf.expand_dims([tgt_tokenizer.bos_id(), reg], 0) # reg포함\n",
    "    for i in range(dec_train.shape[-1]):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = generate_masks(_input, output)\n",
    "        \n",
    "\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = model(_input, output, enc_padding_mask, combined_mask, dec_padding_mask)\n",
    "        \n",
    "        predicted_id = tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()\n",
    "        if tgt_tokenizer.eos_id() == predicted_id:\n",
    "            result = tgt_tokenizer.decode_ids(ids)\n",
    "            return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "        ids.append(predicted_id)\n",
    "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "    result = tgt_tokenizer.decode_ids(ids)\n",
    "    return pieces, result, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2a227c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 번역 생성 및 Attention 시각화 결합\n",
    "def translate(sentence, model, reg, src_tokenizer, tgt_tokenizer, plot_attention=False):\n",
    "    pieces, result, enc_attns, dec_attns, dec_enc_attns = evaluate(sentence, model, reg, src_tokenizer, tgt_tokenizer)\n",
    "    \n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "\n",
    "    if plot_attention:\n",
    "        visualize_attention(pieces, result.split(), enc_attns, dec_attns, dec_enc_attns)\n",
    "  \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61acd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    \"How was your day? I was the best.\",\n",
    "    \"Take your time, please.\",\n",
    "    \"I’m about to leave. Please hold for a moment.\",\n",
    "    \"Have you heard of it?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1caf44a7",
   "metadata": {},
   "source": [
    "## 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7017002",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 학습\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "def train_and_checkpoint(transformer, manager, EPOCHS):\n",
    "    ckpt.restore(manager.latest_checkpoint)\n",
    "    if manager.latest_checkpoint:\n",
    "        print(\"Restored from {}\".format(manager.latest_checkpoint))\n",
    "    \n",
    "    else:\n",
    "        print(\"Initializing from scratch.\")\n",
    "\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        total_loss = 0\n",
    "\n",
    "        idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))\n",
    "        random.shuffle(idx_list)\n",
    "        t = tqdm(idx_list)\n",
    "\n",
    "        for (batch, idx) in enumerate(t):\n",
    "            batch_loss, enc_attns, dec_attns, dec_enc_attns = train_step(enc_train[idx:idx+BATCH_SIZE],\n",
    "                                                                         dec_train[idx:idx+BATCH_SIZE],\n",
    "                                                                         transformer,\n",
    "                                                                         optimizer)\n",
    "\n",
    "            total_loss += batch_loss\n",
    "\n",
    "            t.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "            t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))\n",
    "      \n",
    "      \n",
    "#         # 매 Epoch 마다 제시된 예문에 대한 번역 생성\n",
    "#         for reg in [4,5,6,7,8]:\n",
    "#             translate('I’m about to leave. Please hold for a moment', transformer, reg, enc_tokenizer, dec_tokenizer)\n",
    "            \n",
    "        wandb.log({\"total_loss\": total_loss}, step = epoch)\n",
    "\n",
    "        save_path = manager.save()\n",
    "        print(\"Saved checkpoint for step {}: {}\".format(int(ckpt.step), save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffccc1e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_and_checkpoint(transformer, manager, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13d1d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165d42e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(enc_corpus_test))\n",
    "print(type(dec_corpus_test))\n",
    "print(type(enc_corpus_test_jj))\n",
    "print(type(dec_corpus_test_jj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14460791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jj > 4, jd > 5, gs > 6 , cc > 7 , kw > 8\n",
    "\n",
    "reg = {4 : '제주도', 5 : '전라도', 6 :'경상도',7:'충청도', 8:'강원도'}\n",
    "\n",
    "for i in range(4,9) :\n",
    "    print(f'{reg[i]} 사투리 :')    \n",
    "    translate('what are you doing?',transformer, i, enc_tokenizer, dec_tokenizer)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee30a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jj > 4, jd > 5, gs > 6 , cc > 7 , kw > 8\n",
    "\n",
    "reg = {4 : '제주도', 5 : '전라도', 6 :'경상도',7:'충청도', 8:'강원도'}\n",
    "\n",
    "for i in range(4,9) :\n",
    "    print(f'{reg[i]} 사투리 :')    \n",
    "    translate(examples[2],transformer, i, enc_tokenizer, dec_tokenizer)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8897a7e7",
   "metadata": {},
   "source": [
    "# Step 5. 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8460d3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_corpus_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2ad25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_corpus_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae6314c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_tokenizer.encode_as_ids(dec_corpus_test_jj.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff889cea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "enc_tokenizer.encode_as_ids(enc_corpus_test_jj.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978ec458",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_corpus_test_jj.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f5330c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_corpus_test_jj.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406ff3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "def eval_bleu_single(model, reg, src_sentence, tgt_sentence, enc_tokenizer, dec_tokenizer, verbose=True):\n",
    "    src_tokens = enc_tokenizer.encode_as_ids(src_sentence)\n",
    "    tgt_tokens = dec_tokenizer.encode_as_ids(tgt_sentence)\n",
    "\n",
    "    if (len(src_tokens) > 100): return None\n",
    "    if (len(tgt_tokens) > 100): return None\n",
    "\n",
    "    reference = tgt_sentence.split()\n",
    "    candidate = translate(src_sentence, model, reg, enc_tokenizer, dec_tokenizer).split()\n",
    "\n",
    "    score = sentence_bleu([reference], candidate,\n",
    "                          smoothing_function=SmoothingFunction().method1)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Source Sentence: \", src_sentence)\n",
    "        print(\"Model Prediction: \", candidate)\n",
    "        print(\"Real: \", reference)\n",
    "        print(\"Score: %lf\\n\" % score*100)\n",
    "        \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cdf5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# single test (제주도사투리)\n",
    "test_idx = 70\n",
    "\n",
    "eval_bleu_single(transformer, \n",
    "                 4,\n",
    "                 enc_corpus_test_jj.iloc[test_idx], \n",
    "                 dec_corpus_test_jj.iloc[test_idx], \n",
    "                 enc_tokenizer, \n",
    "                 dec_tokenizer, \n",
    "                 verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb9ced8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_bleu(model, reg, src_sentences, tgt_sentence, enc_tokenizer, dec_tokenizer, verbose=True):\n",
    "    total_score = 0.0\n",
    "    sample_size = len(src_sentences)\n",
    "    \n",
    "    for idx in tqdm(range(sample_size)):\n",
    "        score = eval_bleu_single(model, reg, src_sentences.iloc[idx], tgt_sentence.iloc[idx], enc_tokenizer, dec_tokenizer, verbose)\n",
    "        if not score: continue\n",
    "        \n",
    "        total_score += score\n",
    "    \n",
    "    print(\"Num of Sample:\", sample_size)\n",
    "    print(\"Total Score:\", (total_score / sample_size)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9483787",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_corpus_test_jj.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e24a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null data 제거\n",
    "enc_corpus_test_jj=enc_corpus_test_jj.dropna(axis=0)\n",
    "enc_corpus_test_jd=enc_corpus_test_jd.dropna(axis=0)\n",
    "enc_corpus_test_gs=enc_corpus_test_gs.dropna(axis=0)\n",
    "enc_corpus_test_cc=enc_corpus_test_cc.dropna(axis=0)\n",
    "enc_corpus_test_kw=enc_corpus_test_kw.dropna(axis=0)\n",
    "\n",
    "dec_corpus_test_jj=dec_corpus_test_jj.dropna(axis=0)\n",
    "dec_corpus_test_jd=dec_corpus_test_jd.dropna(axis=0)\n",
    "dec_corpus_test_gs=dec_corpus_test_gs.dropna(axis=0)\n",
    "dec_corpus_test_cc=dec_corpus_test_cc.dropna(axis=0)\n",
    "dec_corpus_test_kw=dec_corpus_test_kw.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b7fa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reg = {4 : '제주도'}\n",
    "eval_bleu(transformer, 4, enc_corpus_test_jj.iloc[:], dec_corpus_test_jj.iloc[:], enc_tokenizer, dec_tokenizer, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ec0c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reg = {5 : '전라도'}\n",
    "eval_bleu(transformer, 5, enc_corpus_test_jd.iloc[:], dec_corpus_test_jd.iloc[:], enc_tokenizer, dec_tokenizer, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e743c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reg = {6 :'경상도'}\n",
    "eval_bleu(transformer, 6, enc_corpus_test_gs.iloc[:], dec_corpus_test_gs.iloc[:], enc_tokenizer, dec_tokenizer, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff7869f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reg = {7:'충청도'}\n",
    "eval_bleu(transformer, 7, enc_corpus_test_cc.iloc[:], dec_corpus_test_cc.iloc[:], enc_tokenizer, dec_tokenizer, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c783906",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reg = {8:'강원도'}\n",
    "eval_bleu(transformer, 8, enc_corpus_test_kw.iloc[:], dec_corpus_test_kw.iloc[:], enc_tokenizer, dec_tokenizer, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae331c0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39cf1dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46da1d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Bleu example\n",
    "# from datasets import load_metric\n",
    "\n",
    "# bleu = load_metric(\"bleu\")\n",
    "# predictions = [[\"I\", \"have\", \"thirty\", \"six\", \"years\"]] \n",
    "# references = [\n",
    "#     [[\"I\", \"am\", \"thirty\", \"six\", \"years\", \"old\"], [\"I\", \"am\", \"thirty\", \"six\"]]\n",
    "# ]\n",
    "# bleu.compute(predictions=predictions, references=references)\n",
    "\n",
    "# # Bleu example\n",
    "# from datasets import load_metric\n",
    "\n",
    "# bleu = load_metric(\"bleu\")\n",
    "# #bleu = load_metric('sacrebleu')\n",
    "\n",
    "# reference = enc_corpus_test\n",
    "# predictions = translate(enc_corpus_test, reg, transformer, enc_tokenizer, dec_tokenizer)\n",
    "\n",
    "# bleu.compute(predictions=predictions, references=reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf702d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib as mpl\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    " \n",
    "# import matplotlib.font_manager as fm\n",
    "\n",
    "# fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
    "# font = fm.FontProperties(fname=fontpath, size=9)\n",
    "# plt.rc('font', family='NanumBarunGothic') \n",
    "# # mpl.font_manager._rebuild()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7013c9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 매 Epoch 마다 제시된 예문에 대한 번역 생성시각화\n",
    "# for example in examples:\n",
    "#     translate(example, transformer, en_tokenizer, ko_tokenizer, plot_attention=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad7ad66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2768c8f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "670px",
    "left": "54px",
    "top": "208px",
    "width": "336px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
